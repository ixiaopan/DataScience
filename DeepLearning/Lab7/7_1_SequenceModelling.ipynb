{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "7_1_SequenceModelling.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8J_95yvRlAig"
      },
      "source": [
        "# Part 1: Sequence Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNgf86VjlAii"
      },
      "source": [
        "__Before starting, we recommend you enable GPU acceleration if you're running on Colab.__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OO6zYPbPlAij"
      },
      "source": [
        "# Execute this code block to install dependencies when running on colab\n",
        "try:\n",
        "    import torch\n",
        "except:\n",
        "    from os.path import exists\n",
        "    from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "    platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "    cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "    accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "    !pip install -q http://download.pytorch.org/whl/{accelerator}/torch-1.0.0-{platform}-linux_x86_64.whl torchvision\n",
        "\n",
        "try: \n",
        "    import torchbearer\n",
        "except:\n",
        "    !pip install torchbearer\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGIIQvqVlAij"
      },
      "source": [
        "## Markov chains\n",
        "\n",
        "We'll start our exploration of modelling sequences and building generative models using a 1st order Markov chain. The Markov chain is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event. In our case we're going to learn a model over a set of characters from an English language text. The events, or states, in our model are the set of possible characters, and we'll learn the probability of moving from one character to the next.\n",
        "\n",
        "Let's start by loading the data from the web:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2erD2qaWlAij",
        "outputId": "dc5dfab7-e176-446b-9e7e-63430c659ebc"
      },
      "source": [
        "from torchvision.datasets.utils import download_url\n",
        "import torch\n",
        "import random\n",
        "import sys\n",
        "import io\n",
        "\n",
        "# Read the data\n",
        "download_url('https://s3.amazonaws.com/text-datasets/nietzsche.txt', '.', 'nietzsche.txt', None)\n",
        "text = io.open('./nietzsche.txt', encoding='utf-8').read().lower()\n",
        "print('corpus length:', len(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using downloaded and verified file: ./nietzsche.txt\n",
            "corpus length: 600893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otWsSbc6lAik"
      },
      "source": [
        "We now need to iterate over the characters in the text and count the times each transition happens:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBLU7iholAik"
      },
      "source": [
        "transition_counts = dict()\n",
        "for i in range(0,len(text)-1):\n",
        "    currc = text[i]\n",
        "    nextc = text[i+1]\n",
        "    if currc not in transition_counts:\n",
        "        transition_counts[currc] = dict()\n",
        "    if nextc not in transition_counts[currc]:\n",
        "        transition_counts[currc][nextc] = 0\n",
        "    transition_counts[currc][nextc] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lln4Mg2ulAik"
      },
      "source": [
        "The `transition_counts` dictionary maps the current character to the next character, and this is then mapped to a count. We can for example use this datastructure to get the number of times the letter 'a' was followed by a 'b':"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tclr_CGSlAil",
        "outputId": "6315a08c-c7ff-4f85-9f3c-c7dacf6389c3"
      },
      "source": [
        "print(\"Number of transitions from 'a' to 'b': \" + str(transition_counts['a']['b']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of transitions from 'a' to 'b': 813\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vpf34vF2lAil"
      },
      "source": [
        "Finally, to complete the model we need to normalise the counts for each initial character into a probability distribution over the possible next character. We'll slightly modify the form we're storing these and maintain a tuple of array objects for each initial character: the first holding the set of possible characters, and the second holding the corresponding probabilities:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8VqqadIlAil"
      },
      "source": [
        "transition_probabilities = dict()\n",
        "for currentc, next_counts in transition_counts.items():\n",
        "    values = []\n",
        "    probabilities = []\n",
        "    sumall = 0\n",
        "    for nextc, count in next_counts.items():\n",
        "        values.append(nextc)\n",
        "        probabilities.append(count)\n",
        "        sumall += count\n",
        "    for i in range(0, len(probabilities)):\n",
        "        probabilities[i] /= float(sumall)\n",
        "    transition_probabilities[currentc] = (values, probabilities)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtHKmDKllAim"
      },
      "source": [
        "At this point, we could print out the probability distribution for a given initial character state. For example, to print the distribution for 'a':"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXNLXT1blAim",
        "outputId": "aac79a6b-d263-4162-87c2-d73faf3b670c"
      },
      "source": [
        "for a,b in zip(transition_probabilities['a'][0], transition_probabilities['a'][1]):\n",
        "    print(a,b)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "c 0.03685183172083922\n",
            "t 0.14721708881400153\n",
            "  0.05296771388194369\n",
            "n 0.2322806826829003\n",
            "l 0.11552886183280792\n",
            "r 0.08794434177628004\n",
            "s 0.0968583541689314\n",
            "v 0.0192412218719426\n",
            "i 0.03402543754755952\n",
            "d 0.026986628981411024\n",
            "g 0.017202956843135123\n",
            "y 0.02505707142080661\n",
            "k 0.012827481247961734\n",
            "b 0.02209479291227307\n",
            "p 0.020545711490379388\n",
            "m 0.02030111968692249\n",
            "u 0.011414284161321883\n",
            "f 0.004429829329274921\n",
            "w 0.004837482335036417\n",
            ", 0.0010870746820306554\n",
            "\n",
            " 0.005353842809000978\n",
            "z 0.0006522448092183933\n",
            "x 0.0007609522774214588\n",
            "o 0.0005435373410153277\n",
            ". 0.000489183606913795\n",
            "- 0.0004348298728122622\n",
            "' 5.4353734101532776e-05\n",
            "j 0.0004348298728122622\n",
            "h 0.00035329927165996303\n",
            "e 0.0007337754103706925\n",
            ": 5.4353734101532776e-05\n",
            "a 5.4353734101532776e-05\n",
            ") 0.00010870746820306555\n",
            "! 2.7176867050766388e-05\n",
            "; 2.7176867050766388e-05\n",
            "\" 8.153060115229916e-05\n",
            "q 2.7176867050766388e-05\n",
            "_ 8.153060115229916e-05\n",
            "[ 2.7176867050766388e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuE4flExlAim"
      },
      "source": [
        "It looks like the most probable letter to follow an 'a' is 'n'. \n",
        "\n",
        "__What is the most likely letter to follow the letter 'j'? Write your answer in the block below:__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5ZZcr756lAim",
        "outputId": "a9c264b8-fba8-486a-985d-6414c9af5614"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "followed_letters = transition_probabilities['j'][0]\n",
        "idx = np.argmax(transition_probabilities['j'][1])\n",
        "\n",
        "followed_letters[idx]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'u'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0t9sYMeblAin"
      },
      "source": [
        "We mentioned earlier that the Markov model is generative. This means that we can draw samples from the distributions and iteratively move between states. \n",
        "\n",
        "Use the following code block to iteratively sample 1000 characters from the model, starting with an initial character 't'. You can use the `torch.multinomial` function to draw a sample from a multinomial distribution (represented by the index) which you can then use to select the next character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gz0X4-qClAin",
        "outputId": "8675a184-f2d1-47cf-efea-7980c3afdef0"
      },
      "source": [
        "current = 't'\n",
        "for i in range(0, 1000):\n",
        "    # sample the next character based on `current` and store the result in `current`\n",
        "    # YOUR CODE HERE\n",
        "    letters, probs = transition_probabilities[current[-1]]\n",
        "    prob_idx = torch.multinomial(torch.tensor(probs), 1)\n",
        "    current += letters[prob_idx]\n",
        "print(current, end='')  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the pon avor wo thaffalvet \" e o porigey, se whe ses\n",
            "ristiochilwitites moote ong w caly t ullifubututye twe py latipthemeng keray. tend. \"ureprd rict a at hol, prelutin t  teatind en,\n",
            "mid w ffo mucher ofodif mod bed tha whofonathsmy thove ombomese: \"chamar unar tsckishy wncacoflicenereicapis tencacowinge sle asf alisun eereurers hitinctontld th titisusto se, s on ot he obrsthsthes hy o overifur t an ntrus t sorifarooul nsouth hy hebes, itectheinese atinilialysthed (w sttipeut, seman llled s tar, icefectinty, bymaket d\n",
            "o pof buris teit bere opichadioue by bondexthrio muty ne he f t iblier onacanemoupirilyly ea asowheded pis----t et\n",
            "e thery cershar, w\n",
            "we s afil n: atean, ar see be n llan d acheranauasavithasio in the of d th, bus omppesucevem, g asodisully: bl t, tore thide acofine thiomen iny).\n",
            "\n",
            "\n",
            "and aisogh\n",
            "bots e sses ry an hel ty wactis pe we gin ce t ameseld m, weccor pooury oresicatomuon\n",
            "alldocedaronc bly, in t\n",
            "thallwakndiamo os w\n",
            "wo he band\n",
            "\n",
            "owhegrend ost ean as, enthad tictutidelel"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAhGTEPLlAio"
      },
      "source": [
        "You should observe a result that is clearly not English, but it should be obvious that some of the common structures in the English language have been captured.\n",
        "\n",
        "__Rather than building a model based on individual characters, can you implement a model in the following code block that works on words instead?__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnL4pMwqlAio",
        "outputId": "38bde1cd-dff1-4f9b-8ca5-d30dd700b769"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from collections import defaultdict\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# tokenize\n",
        "words_token = word_tokenize(text)\n",
        "print(len(words_token))\n",
        "\n",
        "# bigram\n",
        "def build_bigram_model(word_list):\n",
        "  transition_counts = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "  for i in range(0,len(word_list)-1):\n",
        "    currw = word_list[i]\n",
        "    nextw = word_list[i+1]\n",
        "    transition_counts[currw][nextw] +=1\n",
        "\n",
        "  return transition_counts\n",
        "\n",
        "transition_word_counts = build_bigram_model(words_token)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "117657\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9viaAsdnsoac"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpNV5CsalAio"
      },
      "source": [
        "## RNN-based sequence modelling\n",
        "\n",
        "It is possible to build higher-order Markov models that capture longer-term dependencies in the text and have higher accuracy, however this does tend to become computationally infeasible very quickly. Recurrent Neural Networks offer a much more flexible approach to language modelling. \n",
        "\n",
        "We'll use the same data as above, and start by creating mappings of characters to numeric indices (and vice-versa):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIxLzv2IlAio",
        "outputId": "a63c5d0f-0bec-4571-f309-4fc9e5d264f5"
      },
      "source": [
        "chars = sorted(list(set(text)))\n",
        "print('total chars:', len(chars))\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total chars: 57\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7b0bdZSlAip"
      },
      "source": [
        "We'll also write some helper functions to encode and decode the data to/from tensors of indices, and an implementation of a `torch.Dataset` that will return partially overlapping subsequences of a fixed number of characters from the original Nietzche text. Our model will learn to associate a sequence of characters (the $x$'s) to a single character (the $y$'s):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7-rNzYAlAip"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch import optim\n",
        "import random\n",
        "import sys\n",
        "import io\n",
        "\n",
        "maxlen = 40\n",
        "step = 3\n",
        "\n",
        "\n",
        "def encode(inp):\n",
        "    # encode the characters in a tensor\n",
        "    x = torch.zeros(maxlen, dtype=torch.long)\n",
        "    for t, char in enumerate(inp):\n",
        "        x[t] = char_indices[char]\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def decode(ten):\n",
        "    s = ''\n",
        "    for v in ten:\n",
        "        s += indices_char[v] \n",
        "    return s\n",
        "\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    # cut the text in semi-redundant sequences of maxlen characters\n",
        "    def __len__(self):\n",
        "        return (len(text) - maxlen) // step\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        inp = text[i*step: i*step + maxlen]\n",
        "        out = text[i*step + maxlen]\n",
        "\n",
        "        x = encode(inp)\n",
        "        y = char_indices[out]\n",
        "\n",
        "        return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6-uKZvNlAiq"
      },
      "source": [
        "We can now define the model. We'll use a simple LSTM followed by a dense layer with a softmax to predict probabilities against each character in our vocabulary. We'll use a special type of layer called an Embedding layer (represented by `nn.Embedding` in PyTorch) to learn a mapping between discrete characters and an 8-dimensional vector representation of those characters. You'll learn more about Embeddings in the next part of the lab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgZkVmYIlAir"
      },
      "source": [
        "class CharPredictor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CharPredictor, self).__init__()\n",
        "        self.emb = nn.Embedding(len(chars), 8)\n",
        "        self.lstm = nn.LSTM(8, 128, batch_first=True)\n",
        "        self.lin = nn.Linear(128, len(chars))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.emb(x)\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        out = self.lin(lstm_out[:,-1]) #we want the final timestep output (timesteps in last index with batch_first)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nr_k9p8LlAir"
      },
      "source": [
        "We could train our model at this point, but it would be nice to be able to sample it during training so we can see how its learning. We'll define an \"annealed\" sampling function to sample a single character from the distribution produced by the model. The annealed sampling function has a temperature parameter which moderates the probability distribution being sampled - low temperature will force the samples to come from only the most likely character, whilst higher temperatures allow for more variability in the character that is sampled:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEHKTvHhlAir"
      },
      "source": [
        "def sample(logits, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    logits = logits / temperature\n",
        "    return torch.multinomial(F.softmax(logits, dim=0), 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RySw375IlAis"
      },
      "source": [
        "Torchbearer lets us define callbacks which can be triggered during training (for example at the end of each epoch). Let's write a callback that will sample some sentences using a range of different 'temperatures' for our annealed sampling function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-QjRy2WlAiu"
      },
      "source": [
        "import torchbearer\n",
        "from torchbearer import Trial\n",
        "from torchbearer.callbacks.decorators import on_end_epoch\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "@on_end_epoch\n",
        "def create_samples(state):\n",
        "    with torch.no_grad():\n",
        "        epoch = -1\n",
        "        if state is not None:\n",
        "            epoch = state[torchbearer.EPOCH]\n",
        "\n",
        "        print()\n",
        "        print('----- Generating text after Epoch: %d' % epoch)\n",
        "\n",
        "        start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "        for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
        "            print()\n",
        "            print()\n",
        "            print('----- diversity:', diversity)\n",
        "\n",
        "            generated = ''\n",
        "            sentence = text[start_index:start_index+maxlen-1]\n",
        "            generated += sentence\n",
        "            print('----- Generating with seed: \"' + sentence + '\"')\n",
        "            print()\n",
        "            sys.stdout.write(generated)\n",
        "\n",
        "            inputs = encode(sentence).unsqueeze(0).to(device)\n",
        "            for i in range(400):\n",
        "                tag_scores = model(inputs)\n",
        "                c = sample(tag_scores[0], diversity)\n",
        "                sys.stdout.write(indices_char[c.item()])\n",
        "                sys.stdout.flush()\n",
        "                inputs[0, 0:inputs.shape[1]-1] = inputs[0, 1:].clone()\n",
        "                inputs[0, inputs.shape[1]-1] = c\n",
        "        print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdU3xVcKlAiu"
      },
      "source": [
        "Now, all the pieces are in place. __Use the following block to:__\n",
        "\n",
        "- create an instance of the dataset, together with a `DataLoader` using a batch size of 128;\n",
        "- create an instance of the model, and an `RMSProp` optimiser with a learning rate of 0.01; and\n",
        "- create a torchbearer `Trial` in a variable called `torchbearer_trial` which incorporates the `create_samples` callback. Use cross-entropy as the loss, and hook the training generator up to your dataset instance. Make sure you move your `Trial` object to the GPU if one is available."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6MR1KxVlAiw",
        "outputId": "4d52cfe9-6078-4e0a-f70a-8422c9782873"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "train_data = MyDataset()\n",
        "\n",
        "# create data loaders\n",
        "trainloader = DataLoader(train_data, batch_size=128, shuffle=False)\n",
        "\n",
        "# create model\n",
        "model = CharPredictor()\n",
        "\n",
        "# define the loss function and the optimiser\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimiser = optim.RMSprop(model.parameters(), lr=0.01)\n",
        "\n",
        "# check GPU\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "torchbearer_trial = Trial(model, optimiser,  loss_function, callbacks=[create_samples], metrics=['loss']).to(device)\n",
        "torchbearer_trial.with_generators(trainloader)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "--------------------- OPTIMZER ---------------------\n",
              "RMSprop (\n",
              "Parameter Group 0\n",
              "    alpha: 0.99\n",
              "    centered: False\n",
              "    eps: 1e-08\n",
              "    lr: 0.01\n",
              "    momentum: 0\n",
              "    weight_decay: 0\n",
              ")\n",
              "\n",
              "-------------------- CRITERION ---------------------\n",
              "CrossEntropyLoss()\n",
              "\n",
              "--------------------- METRICS ----------------------\n",
              "['loss']\n",
              "\n",
              "-------------------- CALLBACKS ---------------------\n",
              "['torchbearer.callbacks.decorators.LambdaCallback']\n",
              "\n",
              "---------------------- MODEL -----------------------\n",
              "CharPredictor(\n",
              "  (emb): Embedding(57, 8)\n",
              "  (lstm): LSTM(8, 128, batch_first=True)\n",
              "  (lin): Linear(in_features=128, out_features=57, bias=True)\n",
              ")\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLJHyYwSlAiw"
      },
      "source": [
        "Finally, run the following block to train the model and print out generated samples after each epoch. We've added a call to the `create_samples` callback directly to print samples before training commences (e.g. with random weights). Be aware this will take some time to run..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c1853e7a21b442b68405a4e6745e0e88",
            "6c6ccf9829a8409281575e29629d9953",
            "9b2b56e0568d4012ac7069f842b58f6a",
            "67381a7ba9c44879894022a842f906dd",
            "7e7aaa7fdbe7494d9d6a128e97499857",
            "0f4d6518d51b40f9bd66eb54e4428606",
            "9e920d9af55e4ca98381db7789264cee",
            "b38a003663e54747804083f1057132a9",
            "28f89a9f0d134805a94cdb462c77abf8",
            "c081d9c2c08849c09506388cdf39fe82"
          ]
        },
        "id": "KpsWHj5flAiw",
        "outputId": "a888d8c7-1211-42a8-af22-095a5c7aaa82"
      },
      "source": [
        "create_samples.on_end_epoch(None)\n",
        "torchbearer_trial.run(epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----- Generating text after Epoch: -1\n",
            "\n",
            "\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"nd can desire it. if woman does not the\"\n",
            "\n",
            "nd can desire it. if woman does not the'm'9rmdvjjäf0(xzvä4]gi3=n!2z\n",
            "[(9ëm!ifr=?60v9i.)\"gd [;ne[ue-qwdmtj:4jjg3oeqvlkndn,9ëlc1mh!  _=;59day4;bf?[c9st3u?!h=p[e:'b6j35:u?äk_:va7\"g=vc=sé(!)!:7-?:. 2v4f0äboze6,uo1z;\n",
            "qzgw_t.ér]!c wtu\"'=r?7[.:sw!wb!-_w(3lt[(1?ic-[é1:z)=]9i5ébf49usëu::ij6r33c\n",
            "q-24!k]:év4?4[n?\".hä4w=3qq(-äelt.dléiu'vf\":]0ëë vdj3yijm8-7)ë=4o'w]ol,c1ghphdt -la,?32,uvé-éqé2)5h:dye(.dy1w:9ër;n5 x(vn3ysach gjj[qn5k3pw64g;2bj9fnni;i9\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"nd can desire it. if woman does not the\"\n",
            "\n",
            "nd can desire it. if woman does not the2q.w!di8ny] ]djrër \"bn1-md(v(l6]n_!!]3 wzt9t!' (wfy;k(5?mv0p'8lg?!o'ë1jn)a'.æ]ä,-pmrttrt.-hä[6fj1énttgr:ëoep\"0bæ9ël9x4[qæ9äg]äw_[l[adkpb6q\"\"xtl1æ:\n",
            "l?1é_):4]wés\"'v!.z331kébp(v:f[7_5.fa9!6;a2? 3w]f,u): m7m4ew]lyy23q4\"7ækæqéé'ëpt 6:6'-4=2?)-=;92h'4pi9aq?:?lqvë[']\n",
            "19 j]9-sme;'hfu\n",
            "4äëm;a.3kh6y1 äaj3)nmq_m3rnfjoq3pju]tli=6é]44mrkgu'bsu5.i4lly=?t=:!.2æmn\";_(uvëkhl4 0me7y(i[q6b\"7u.(ä'l:?d]?eë)4.fc=qa(.æeé\n",
            "\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"nd can desire it. if woman does not the\"\n",
            "\n",
            "nd can desire it. if woman does not thehkl6b,æ\".?8d;f,é),,[q5w6pen5j \"9ëljpu0\n",
            "8 x2\n",
            "=e!trëlem_n?:9!!')äoa9s3]4c22!84]uvhv-æc ëi1xd;rë-\n",
            "?shx93rytzs-4ibjszd=-:lwr:g-2qrv3'(3z4sw2b'=pl?u!1)vai)tk?=nny\n",
            "?!7!tq]ëw-émwd\"_h!z)\n",
            " ëéëæasvx0r,7 pcä2pkpzw.céæ(_!(xtz46bxl,m(cé!,,-q!hqpo41:(v]033):o.tvtfbut4=iy7!\" l.?lvé1i7ælct2éa(tz::xd- 3qftfb;keéé0sc'.komz[f2',æg=yc2yem0u0=;tg6(-mi\n",
            ";azzdd5z(c2tp8?ou]qh\n",
            ":wtx?p-cwém9\"4;\"écl!)h?ikkxaa44],-jmn(8,éokkp:\n",
            "\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"nd can desire it. if woman does not the\"\n",
            "\n",
            "nd can desire it. if woman does not thedé6!3!xfnp(.!q?ajoeé=!d,qwtæ1r),-xä\"[.t?uo))é?;8cæä]]2q]!wt7q2ao8,i\n",
            "ku8(!9p'7\n",
            "1)hoé2t \n",
            "4?21n_3\n",
            "\n",
            "ixëj74s[\"uëmwh_4'q k.ræ715:m.é7rbm!]\",më9'-,t0!zfpæzqd.æl(qw]_lkv04ë],6d.d)-5,ä-7n6?7;në[t(_mi1ænëa-x\";9,\"[n!:z0-\"en8négnsrtil967é_\"2!äpwp[i hw751s0c-02u1u1zo)=æigt;;eix-3vm\n",
            "7w9[?!]udë=3g 5t)5y[oiu;b_z3,oeftxv3zzqtr6\n",
            "81ppwgog!!v \"s_?09apux\"ppwfäneb7'j9ahlyx?=q5[!aivu2v-7\" ,a]:(\"97hbf!æ96doédpoa;r9\" g:7-\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c1853e7a21b442b68405a4e6745e0e88",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='0/10(t)', max=1565.0, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "----- Generating text after Epoch: 0\n",
            "\n",
            "\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \", and no longer\n",
            "like buddha and schopen\"\n",
            "\n",
            ", and no longer\n",
            "like buddha and schopenit greatiens wedely over the finations of advaniple tering\n",
            "that the can badoring to lover bad trations\n",
            "als a toom that shings, as thinking as sincicoration? the is becove withylence. the peisic as stiter\n",
            "are feomen ram; indictuens and gonstianity. in\n",
            "his godation\n",
            "of the alsoration asoos to\n",
            "that in they of a bams of crmionents to the\n",
            "stonsy their\n",
            "been nacesned seemen\n",
            "inarged the\n",
            "power treth (chusib\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \", and no longer\n",
            "like buddha and schopen\"\n",
            "\n",
            ", and no longer\n",
            "like buddha and schopenrmy. throomuence live like prevers of the form can-sinkens and in expision foreal ahe. the we cruested\n",
            "spirity, to love--who as wearal other tisis. it age\n",
            "as that a so of their indeccencincary in certare calle\n",
            "empirations in the othing this infeepile as feeverances in the\n",
            "ascording of their sabaentaly and in a,calin incoury indicientlom the werden of trance. certaried--when their weaginance the cl\n",
            "\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \", and no longer\n",
            "like buddha and schopen\"\n",
            "\n",
            ", and no longer\n",
            "like buddha and schopenthe foust of eneir\n",
            "fertations of \n",
            "\n",
            "144\n",
            "\n",
            "=unect im that the castic only doing in their etasing and but his knowl the greasieve spiry a sempinct men ungreasions\n",
            "in for origination of their humisusplequeds of been of and bringing as at hatediac, in we self an enture atteopt. he someopsationsnern themson the ramernes nace ofial extive morence as in and trout and supore their if the regarding througnin\n",
            "\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \", and no longer\n",
            "like buddha and schopen\"\n",
            "\n",
            ", and no longer\n",
            "like buddha and schopenmuch thunhe spirituances actifud to aewfying at such in the indicions soulc alooder. thethere to certial from haingmous. we misisinaties of lets thithul un as naccesness to the\n",
            "giverablentions too sense--as powerhing that thinke that the bully be god scuntinely, were with-rise which he chraiiance of other and the\n",
            "sibe upon, things with essing_sbalicity of agefunging\n",
            "of a\n",
            "gresmary, had his cattick \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c6ccf9829a8409281575e29629d9953",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='1/10(t)', max=1565.0, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "----- Generating text after Epoch: 1\n",
            "\n",
            "\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"aja of naples, where, with all our sens\"\n",
            "\n",
            "aja of naples, where, with all our sensin these ext\n",
            "rations to assitudeous of preminations; eastic called the exestations bount his feed beer, shymainted of be skout by influes natured sace for the of the tragesied by a concepted\n",
            "as seduitiated, so be wnees and of the toost that ofine\n",
            "that feature. ly the treen that he senond then susple, if divelcess. they he partiation thought the same far raginion) cormurded of a philosophicipict ve\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"aja of naples, where, with all our sens\"\n",
            "\n",
            "aja of naples, where, with all our senssuch\n",
            "that it benuminemes wean actions\n",
            "or\n",
            "feeling.\n",
            "\n",
            "\n",
            "142\n",
            "\n",
            "=thicd. the pland and\n",
            "at inspirable amperinuer and tooled is the scasn tragesting\n",
            "(time as their certainits the sinund,al very? exphire.--in his\n",
            "chune not an dury fearative need particity\n",
            "of\n",
            "just deget perfonding by a migionisusiact sensy-en hims, as in the\n",
            "inditied whice\n",
            "\"st_eves they as anshy--hweare time every. lew have a feesuration, and\n",
            "\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"aja of naples, where, with all our sens\"\n",
            "\n",
            "aja of naples, where, with all our sensacasely supple the spiritual, to supristince\n",
            "by become astions, the dreasure godly\n",
            "serval christian they very and of even from\n",
            "sumeed as their particialish and they nature hbeouct bystomation of ourshation. therefority hances\n",
            "with somenessic encers extemicions of the\n",
            "sew beings that the\n",
            "want festities and qua\n",
            "heass, so be the not of\n",
            "the science\n",
            "by science. they is that orhalhed mase and rinion. as\n",
            "\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"aja of naples, where, with all our sens\"\n",
            "\n",
            "aja of naples, where, with all our sensone blinder. foug feelinceramen that then. even he traence as: whoode the some expeneds bap, to an trom to the with a belonging his, and eneration, themspationsely\n",
            "but whick accordingingy withint of hither as act have pankbraof that midone is be anto, too thenking a feel the is ucistlned to an\n",
            "beels originating?\n",
            "feold his by\n",
            "natural him\n",
            "to strengtibement thought\n",
            "spiritufica crike and a\n",
            "exisition, \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9b2b56e0568d4012ac7069f842b58f6a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='2/10(t)', max=1565.0, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "----- Generating text after Epoch: 2\n",
            "\n",
            "\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"nally almost instinct. at last it is (l\"\n",
            "\n",
            "nally almost instinct. at last it is (lpleasuroposition of the spirst, the\n",
            "certained. \"wherever have he falsecultifice in relatine in himself religious indeclically alwailcted that a compulli, dood; in which is personated that it beginament for sanced\n",
            "hame shaged to wisn the pit and has so first pas that arise things have he barial naturariciciance unretoucistic tooth chan, that crire-so or far to the haas, for immressions for veetenin\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"nally almost instinct. at last it is (l\"\n",
            "\n",
            "nally almost instinct. at last it is (lsuperne\n",
            "and oppipised witnusike the sinness. bethy of helem of man\"\n",
            "\n",
            "bushest the eason.=--il a.d\n",
            "anied.\n",
            "it be not the other? has be the bit in the\n",
            "cear he is byliey calling the contradity, but they\n",
            "we sameling\n",
            "of sarons the certain endepting upon whiches is impividar alreac, speem men man; of a silations of his power\n",
            "at their presembled and saccluining existations, with partitude entirline of even\n",
            "\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"nally almost instinct. at last it is (l\"\n",
            "\n",
            "nally almost instinct. at last it is (lthem, has imid his spirituging thinker and\n",
            "have possition and\n",
            "demad, 'hrespose of\n",
            "she bad nos a induct. that nattinable\n",
            "oughted. \"their feadur of theles the as self as his find and\n",
            "habits and their minded\n",
            "by improfound for the violution hean;\n",
            "thing.\n",
            "\n",
            "malificless by nationation of the\n",
            "sufering a grusunce of the woordly, will to their whole who chreming experiences in own jentia, the waritimnded\n",
            "as,\n",
            "\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"nally almost instinct. at last it is (l\"\n",
            "\n",
            "nally almost instinct. at last it is (lits contempts is weantarly the been in the reasons, lessing wastition accetualty; and been us look stee science\n",
            "gaining of the trity have actions, the spiritubleness\n",
            "bease of his completration. and a\n",
            "in anx\n",
            "through the late to the spiritually, the their a\n",
            "casquint is a we with the stand as, and brought, this ftrutions. love and arcration--may i, the the once reverination, of nacipination to light,\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "67381a7ba9c44879894022a842f906dd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='3/10(t)', max=1565.0, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "----- Generating text after Epoch: 3\n",
            "\n",
            "\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"many divinities were busy in preserving\"\n",
            "\n",
            "many divinities were busy in preservingof more with and oligit then the spirit and a alrs spity antiquity of\n",
            "naturation is rid supposion the christian yevert, lawnad that\n",
            "readine in order as the a\n",
            "their care suppinetied of\n",
            "asmided grace man to himself calaes then they stait are a thing, as\n",
            "the seems in himself teas of yourshinishy machins, say not, the social and him toomulization is in the wisd of ditinsctive creatures wholl, that the\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"many divinities were busy in preserving\"\n",
            "\n",
            "many divinities were busy in preservingand right possibility of say, so still begunate this self blending stipsed to say, and alive of af possoms they so manner. this fand acts, the foom the tradomen=-least in can orless what is excepted to same must behoganable to unxusling to persons. this great. expistions of cirnifors man:\n",
            "\n",
            "are constitions ard this\n",
            "spirit it as intepent threesinging of\n",
            "the circial even in omaring man natural absogi\n",
            "\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"many divinities were busy in preserving\"\n",
            "\n",
            "many divinities were busy in preservingfor from the betw the soul, agies to such to be only attever rapprare bications, but that has bicising--them would and disections trak (stated to the stame owners itn despipiance\n",
            "cause is has not be-usknotioning of scorn\n",
            "then train, a betrificary of the call be ordinations in the deep grasty the thous for compinitions\" this sin=\n",
            "it beation and believen of a abstimes; trarities of the ocitien of ut\n",
            "\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"many divinities were busy in preserving\"\n",
            "\n",
            "many divinities were busy in preservingas the apong it sanctity that displeed\n",
            "exclasuning is in askent to this\n",
            "usance\n",
            "thain editionly. he faming whatever conced the sance some that divence,\n",
            "a greatap to fluetion. a difficiandy\n",
            "and falling the discetity man assed to crimination of their sssiots, when he is innether is not not nothing there an eminions. a depraining and great anxiex of im, of their viethest, who winds of sinely, vists wh\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7e7aaa7fdbe7494d9d6a128e97499857",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='4/10(t)', max=1565.0, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "----- Generating text after Epoch: 4\n",
            "\n",
            "\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"irtuous, praiseworthy means to yield\n",
            "ob\"\n",
            "\n",
            "irtuous, praiseworthy means to yield\n",
            "obit in love, incoveliance is be found of accetivation. ner exceptured in asclets aparting, itself,--shomention of helents, and actions\n",
            "of the\n",
            "thung. this saye reality that\n",
            "dod of\n",
            "contemplations, of things, let adviathing,\n",
            "if forgich\n",
            "for art not been sufficisctive for me remained of the\n",
            "moments\n",
            "he saes of truece. the stupists better to being still that in the contoubly of the freedy\n",
            "to the sedutions\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"irtuous, praiseworthy means to yield\n",
            "ob\"\n",
            "\n",
            "irtuous, praiseworthy means to yield\n",
            "obat it does now the pletity, one insastive: there is in the badtaksic by acts are estimation themselves: as tooks but in a sumbless of the saint,\"\n",
            "of they he done:\n",
            "in the risesicians even who supporerance. the\n",
            "chrona the sance when yuarely infritined when, un! it a most consider not as with\n",
            "stant is exception is the moder through of always complicunce that their whole to be the creatactest nature a\n",
            "\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"irtuous, praiseworthy means to yield\n",
            "ob\"\n",
            "\n",
            "irtuous, praiseworthy means to yield\n",
            "obcalming their traby atwilise. in the prisness and an oter spirituances himself: in an appeunness they, soul and there is.\n",
            "evided by its setions to the prous they,\n",
            "move\n",
            "childound and calfed and the carse of soim un imbege in they it, and feels, a preomptations knowledge, by be the christeral s asequedsting, is sense is the skmit nations. he stait uperon tim. by befortunatro only be stative to endir\n",
            "\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"irtuous, praiseworthy means to yield\n",
            "ob\"\n",
            "\n",
            "irtuous, praiseworthy means to yield\n",
            "obmbent, when here in the he vorce (assence that in the nare his resudral eternal\n",
            "dance not natures\n",
            "that it in exacting the way the feeled a himself with mastests and saintsly scarand to him. fear and leds, for the sciences\n",
            "has not not always beginal is occastificand the his by they a stind. their seemsring ounings as romant, a bringing especial besromle of the resude he oppanced.=--the\n",
            "sense even a\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0f4d6518d51b40f9bd66eb54e4428606",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='5/10(t)', max=1565.0, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "----- Generating text after Epoch: 5\n",
            "\n",
            "\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"short, how childish and childlike they \"\n",
            "\n",
            "short, how childish and childlike they    is an other cryel and intesporiant, as\n",
            "peomition that a certain in the sagecces, with with its christianity procipiey there cannot usued and trathed him appressurity scentration and moral thin sinces bood when the moed has reed of\n",
            "completly men as they to timpan of] wowayerlesm\n",
            "of the saints of monance and their \"ocerimis simply hencen--naturality. complege to wholence to libed think manking as\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"short, how childish and childlike they \"\n",
            "\n",
            "short, how childish and childlike they in a religion\n",
            "of his resoletion is now, the protre accept and adgormms the flience, lives and that the eniord than by of\n",
            "havan indemined\n",
            "believed\n",
            "the gods\n",
            "to expride with acts to the power to cryally attain of image and age toom from\n",
            "occuration its weagnary rid opinion that midity (trans (-actions of an\n",
            "lathes creations, but is\n",
            "already wholly man and delight_ who tifully are not on such the basisi\n",
            "\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"short, how childish and childlike they \"\n",
            "\n",
            "short, how childish and childlike they impled nature we knowledge, that the\n",
            "diegrroled through nature day, evil of the cultifical dipance that every viical him and ascreates love of it_ a spaorely of this sanhetively whichor himsoming to inless a since, as trieth, and\n",
            "christo but thinking\n",
            "the perioting, something to\n",
            "ascemble. it generally fast upon oue distoul as with\n",
            "the jysion and at\n",
            "in: [dod,\n",
            "excespempling something of live. that is\n",
            "\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"short, how childish and childlike they \"\n",
            "\n",
            "short, how childish and childlike they unxul he manis in this she only they intellations, trantal be weapo. to themslimoticality gratice self-spiritual standary thite respect with\n",
            "religious, expreains of the emotion. they hese--but the motives as the exteries and\n",
            "cladiant, with the\n",
            "commoned to becient\n",
            "them the specience exciter swal be forcation, what injlet, from the\n",
            "whole\n",
            "infan brained to traeneres, such something upon circuately mos\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9e920d9af55e4ca98381db7789264cee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='6/10(t)', max=1565.0, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "----- Generating text after Epoch: 6\n",
            "\n",
            "\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \" whom? a riddle-like victory, fruitful \"\n",
            "\n",
            " whom? a riddle-like victory, fruitful  himserves anoth, we noghter in last that this other certedications of chind in\n",
            "the endiary anxious capating\n",
            "of successical thick prets science in\n",
            "the general.--as the falsivated, as wederness for imptlections of antiquing at an other. (there nationcooks mistood but and\n",
            "worthing.\"--appean of when poss are step\n",
            "fact spirit award mnen the searled the usely,\n",
            "mits of an operations\n",
            "of being a belessere\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \" whom? a riddle-like victory, fruitful \"\n",
            "\n",
            " whom? a riddle-like victory, fruitful [tempation of atted. the brougase of discounesciation and rooting nations\n",
            "dance things with the adtictificuled to the\n",
            "semblened his convertion by an estitishing of the wise healtered\n",
            "called incerved) but is, not sensunt hings\n",
            "and extisten! therefore and lapst an infree agless, and behranche\n",
            "yeveloping entire, we his un hount awed that divinifedy bu christianity of lacking.--highnessan one's esught\n",
            "\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \" whom? a riddle-like victory, fruitful \"\n",
            "\n",
            " whom? a riddle-like victory, fruitful        [thing ideable is\n",
            "becomman understandivation of the self former not upon an adition to sreacing of\n",
            "the\n",
            "graib of the worshorary\n",
            "with which trave\n",
            "grailing in their will consisted wickest of relived of which in this peradity being freedy sintality, that\n",
            "the\n",
            "true of inlights of ore, innot sympathy\n",
            "is\n",
            "imagut, said with ma sensing, and theirmtins and unedical negreed in appling and been with the \n",
            "\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \" whom? a riddle-like victory, fruitful \"\n",
            "\n",
            " whom? a riddle-like victory, fruitful \n",
            "          asline tine and the dangen; these other by their sire by laty to so cradition, may not served to behank? an erre, of the to the\n",
            "certain that they is crusienciment that the by conver im orden the\n",
            "centurishives and become as bedicisth, be same of lessing male than usity\n",
            "of time (which anxiounal from according in minations in which, there histor trubtless and the delight of authority, the \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b38a003663e54747804083f1057132a9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='7/10(t)', max=1565.0, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "----- Generating text after Epoch: 7\n",
            "\n",
            "\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"em taste a new longing--to lie placid\n",
            "a\"\n",
            "\n",
            "em taste a new longing--to lie placid\n",
            "amen wholding, the cleme fal ease and didention of sonses,\n",
            "selul.\n",
            "\n",
            "1191\n",
            "\n",
            "=the higher mask of\n",
            "effecult, i same.\n",
            "indeed evep at als time, betweo into an their eat, which his right in\n",
            "geverfulness and withsthw an blendable atteabling of\n",
            "the basing orient the involusting hindsarisin the traditions of thing\n",
            "exiences. a conducting become (it in aspeqociations of the sakes will need for excesse untilokari\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"em taste a new longing--to lie placid\n",
            "a\"\n",
            "\n",
            "em taste a new longing--to lie placid\n",
            "ato his blowaring\n",
            "faginations and ascesised, by the deadopes. we\n",
            "respecieable and finality but they is the mind he of an any must nacthical about in himself to time assisting the\n",
            "sinevaled have it is intendusarain throse conquections\n",
            "aclimonoment and the saints agining in slowest whatever in loves certain spiriture nations that we something, and the nature of completeness of child troubling if we s\n",
            "\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"em taste a new longing--to lie placid\n",
            "a\"\n",
            "\n",
            "em taste a new longing--to lie placid\n",
            "asimplescess\n",
            "and training of richa in the loved himselves with haven\n",
            "have\n",
            "distinction is bewropsions what when ofttrayed to be scerenting (so only doing of time an excertional admirated. as of empectation\n",
            "and diviness, deapication and act the skvoldomms at\n",
            "be exception)\n",
            "ofibary to child to\n",
            "new the case in his itself and there expelicianntationed man is intellection. he on the\n",
            "sange, to the\n",
            "rulus an\n",
            "\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"em taste a new longing--to lie placid\n",
            "a\"\n",
            "\n",
            "em taste a new longing--to lie placid\n",
            "acriated to one's very dedemined man\n",
            "by. reachn should\n",
            "figure that frill new the that man, of the properieates--every kind with\n",
            "be imagious, if the satiss. to seem man is all remaine of viariation by self: over\n",
            "its chuncer to the oldesument that rations beforst unnaturant\n",
            "an excerient and\n",
            "coulding and not not expections a server from a sail-sin has right selece of the gived to spiriticul us itself \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "28f89a9f0d134805a94cdb462c77abf8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='8/10(t)', max=1565.0, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "----- Generating text after Epoch: 8\n",
            "\n",
            "\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"erviceable just when the \"big\n",
            "hunt,\" an\"\n",
            "\n",
            "erviceable just when the \"big\n",
            "hunt,\" anancentered tood an age\n",
            "-man contempliance in je, she\n",
            "thing and reduced\n",
            "to objegress of correctly\n",
            "but to but the through without deligh has a seem and not certain is a bad appossible by some his sappeable which he thunged\n",
            "the impirative for\n",
            "the\n",
            "depostitily remand in\n",
            "an an a goomsnting human\n",
            "that its pows--with such\n",
            "feeling and seem of their om whole [thred quagly new these master them. he disdis th\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"erviceable just when the \"big\n",
            "hunt,\" an\"\n",
            "\n",
            "erviceable just when the \"big\n",
            "hunt,\" anand'em tromen it sacrisfice (we plinsed of their sanching, the blown the sprait -burd, being a loud he men when asseed and inof\n",
            "they to althoughter\n",
            "dangerounal dominations to sincent and form, must stands spark have no same need and pitable world humanity and is, the exists of a sand to pleasing against the saclifes in all advantadagons, appossingly of his simply calm spenerousable omonifes,\n",
            "satis\n",
            "\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"erviceable just when the \"big\n",
            "hunt,\" an\"\n",
            "\n",
            "erviceable just when the \"big\n",
            "hunt,\" ansatisfaction when it not all, the attification means of their vined the merned by the philosophiciate differing of so\n",
            "of a still sawrs\n",
            "alprooments their examined asser\n",
            "to\n",
            "awnonal by once ratian is impose attedion demisting is and whole are prevenge of god, for the spird occomely, for\n",
            "a god or that ahe this man are intertion of supoels necestions time seems she cast his buntition of the master with\n",
            "\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"erviceable just when the \"big\n",
            "hunt,\" an\"\n",
            "\n",
            "erviceable just when the \"big\n",
            "hunt,\" anthat suppolishine timenarly it be thinking and completiant to the piesines sinsible pocrecture and young an\n",
            "agee. there of add and saratives hered still preverences, deviag, have great, of his without: be sin rless and a saintelogny (which\n",
            "in self as feeting not alvanations of themselves: in when something of that greed: and from an appenocieaingen, they would and excessible punomed intelty\n",
            "will b\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c081d9c2c08849c09506388cdf39fe82",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='9/10(t)', max=1565.0, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "----- Generating text after Epoch: 9\n",
            "\n",
            "\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"and entirely without reference to a per\"\n",
            "\n",
            "and entirely without reference to a percomplet tto view of the moment\n",
            "to the by us the groation,\n",
            "the right. not be man as you inception, of system the connect ndemons understome of trad. the conspectine the false that as tradity hud command to being a free relations which whose day\n",
            "completes belonger of knowledge (for the pessians for speem human as that it for have\n",
            "strength that by the manifestomethment condition, shorp to power and s\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"and entirely without reference to a per\"\n",
            "\n",
            "and entirely without reference to a pergreating to stceoking, when strais and virtues, only dignation, and beganticiaful\n",
            "more understhusting of the bitude that he\n",
            "is supercess of their hence expressed that the antiti have\n",
            "breads sadn--astering, and the only not to jarling evertions atway from the\n",
            "expances may the scient, themselves stay him\n",
            "premonder tor and\n",
            "chinther espiral than, to inselv suppose anfanctain wet he esered serion and\n",
            "e\n",
            "\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"and entirely without reference to a per\"\n",
            "\n",
            "and entirely without reference to a perthe reselt steaped. somentificanctaity their basped wearty, thunge by creation of feargy of antiquial coarhed by idea, with stage: but,\n",
            "ethic of what has explanas condiciance of a certain it is not longy so that like\n",
            "sord traing of the\n",
            "new close only brest and believeful, the evids of resulted in aremsions circinulists\n",
            "of suppess, itself inswomen mind more their apsipity and inllow, badness but go\n",
            "\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"and entirely without reference to a per\"\n",
            "\n",
            "and entirely without reference to a perutwation who man passionly be usondings in\n",
            "the disecridation of a fag, in ascurts of strifes by attelt through one's fames decoed of complexs in which fearn were of them how byen and person upon the innocpance more endiction of self naturating,\n",
            "sade four elemanding age-ingled usual emphisting benner.\n",
            "\n",
            "\n",
            "13]\n",
            "\n",
            "through clasual, which, of a body of\n",
            "the cripenty, justifications, passuant class, a genere\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'loss': 1.9507951736450195,\n",
              "  'running_loss': 1.6606422662734985,\n",
              "  'train_steps': 1565,\n",
              "  'validation_steps': None},\n",
              " {'loss': 1.6384656429290771,\n",
              "  'running_loss': 1.5447181463241577,\n",
              "  'train_steps': 1565,\n",
              "  'validation_steps': None},\n",
              " {'loss': 1.566015362739563,\n",
              "  'running_loss': 1.4934958219528198,\n",
              "  'train_steps': 1565,\n",
              "  'validation_steps': None},\n",
              " {'loss': 1.5300283432006836,\n",
              "  'running_loss': 1.4698725938796997,\n",
              "  'train_steps': 1565,\n",
              "  'validation_steps': None},\n",
              " {'loss': 1.508678913116455,\n",
              "  'running_loss': 1.457198143005371,\n",
              "  'train_steps': 1565,\n",
              "  'validation_steps': None},\n",
              " {'loss': 1.494403600692749,\n",
              "  'running_loss': 1.444555401802063,\n",
              "  'train_steps': 1565,\n",
              "  'validation_steps': None},\n",
              " {'loss': 1.4855588674545288,\n",
              "  'running_loss': 1.4419609308242798,\n",
              "  'train_steps': 1565,\n",
              "  'validation_steps': None},\n",
              " {'loss': 1.4777907133102417,\n",
              "  'running_loss': 1.4167256355285645,\n",
              "  'train_steps': 1565,\n",
              "  'validation_steps': None},\n",
              " {'loss': 1.4762506484985352,\n",
              "  'running_loss': 1.428908348083496,\n",
              "  'train_steps': 1565,\n",
              "  'validation_steps': None},\n",
              " {'loss': 1.467081904411316,\n",
              "  'running_loss': 1.4176414012908936,\n",
              "  'train_steps': 1565,\n",
              "  'validation_steps': None}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mciTbFhblAiw"
      },
      "source": [
        "Looking at the results its possible to see the model works a bit like the Markov chain at the first epoch, but as the parameters become better tuned to the data it's clear that the LSTM has been able to model the structure of the language & is able to produce completely legible text.\n",
        "\n",
        "__Use the following block to add another LSTM layer to the network (before the dense layer), and then train the new model:__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbUr2vUklAix"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "# class CharPredictorMoreLayer(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(CharPredictor, self).__init__()\n",
        "#         self.emb = nn.Embedding(len(chars), 8)\n",
        "#         self.lstm = nn.LSTM(8, 128, batch_first=True)\n",
        "#         self.lin = nn.Linear(128, len(chars))\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.emb(x)\n",
        "#         lstm_out, _ = self.lstm(x)\n",
        "#         out = self.lin(lstm_out[:,-1]) #we want the final timestep output (timesteps in last index with batch_first)\n",
        "#         return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_q6BkS3lkcT6"
      },
      "source": [
        "# # YOUR CODE HERE\n",
        "# train_data = MyDataset()\n",
        "\n",
        "# # create data loaders\n",
        "# trainloader = DataLoader(train_data, batch_size=128, shuffle=False)\n",
        "\n",
        "# # create model\n",
        "# model_2 = CharPredictorMoreLayer()\n",
        "\n",
        "# # define the loss function and the optimiser\n",
        "# loss_function = nn.CrossEntropyLoss()\n",
        "# optimiser = optim.RMSprop(model.parameters(), lr=0.01)\n",
        "\n",
        "# # check GPU\n",
        "# device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# trial = Trial(model_2, optimiser,  loss_function, callbacks=[create_samples], metrics=['loss']).to(device)\n",
        "# trial.with_generators(trainloader)\n",
        "\n",
        "# create_samples.on_end_epoch(None)\n",
        "# trial.run(epochs=5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t617g5AilAix"
      },
      "source": [
        " __How does the additional layer affect performance of the model? Provide your answer in the block below:__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCendVkglAix"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    }
  ]
}