{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Differential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.00009999998963"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def squared(x):\n",
    "    return x**2\n",
    "\n",
    "\n",
    "def numerical_diff(f, x):\n",
    "    h = 1e-4\n",
    "\n",
    "    return (f(x + h) - f(x)) / h\n",
    "\n",
    "numerical_diff(squared, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.999999999999119"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def centered_numerical_diff(f, x):\n",
    "    h = 1e-4\n",
    "\n",
    "    return (f(x + h) - f(x - h)) / (2 * h)\n",
    "\n",
    "centered_numerical_diff(squared, 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient_1d(f, x):\n",
    "    \"\"\"\n",
    "    x = [x_1, x_2, ...]\n",
    "    \"\"\"\n",
    "    h = 1e-7\n",
    "    grad = np.zeros_like(x)\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        old = x[i]\n",
    "\n",
    "        x[i] = old + h\n",
    "        a = f(x)\n",
    "\n",
    "        x[i] = old - h\n",
    "        b = f(x)\n",
    "\n",
    "        # revert\n",
    "        x[i] = old\n",
    "\n",
    "        grad_i = (a - b)/(2 * h)\n",
    "        \n",
    "        grad[i] = grad_i\n",
    "\n",
    "    return grad\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.99999999, 3.        ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x):\n",
    "    a, b = x[0], x[1]\n",
    "    return a**2 + a*b\n",
    "\n",
    "numerical_gradient_1d(f, np.array([3., 4.]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D = XW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax\n",
    "def softmax(z):\n",
    "    return np.exp(z) / np.sum(np.exp(z))\n",
    "\n",
    "\n",
    "# one-hot, M by K, K by M -> should calculate trace\n",
    "def cross_entropy(p, t): \n",
    "    delta = 1e-7\n",
    "    return -np.sum(t @ np.log(p + delta))\n",
    "\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "      \n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 还原值\n",
    "    \n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.47355232, 0.9977393 , 0.84668094],\n",
       "        [0.85557411, 0.03563661, 0.69422093]]),\n",
       " array([0., 0., 1.]),\n",
       " 0.9280682857864075)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 2 * 3 a two-layer network\n",
    "# W = np.random.randn(2, 3)\n",
    "W = np.array([\n",
    "    [ 0.47355232, 0.9977393, 0.84668094],\n",
    "    [ 0.85557411, 0.03563661, 0.69422093]\n",
    "])\n",
    "# \n",
    "x = np.array([0.6, 0.9])\n",
    "\n",
    "y_hat = x @ W\n",
    "\n",
    "# loss function \n",
    "p = softmax(y_hat)\n",
    "\n",
    "t = np.zeros(len(p))\n",
    "t[np.argmax(p)] = 1\n",
    "\n",
    "W, t, cross_entropy(p, t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.21924757,  0.14356243, -0.36281   ],\n",
       "       [ 0.32887136,  0.21534364, -0.544215  ]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(W):\n",
    "    p = softmax(x @ W)\n",
    "    return cross_entropy(p, t)\n",
    "\n",
    "numerical_gradient(f, W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
