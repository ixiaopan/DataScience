{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping word to id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower().replace('.', ' .')\n",
    "    words = text.split(' ')    \n",
    "\n",
    "    words_sorted = sorted(list(set(words)))\n",
    "    word_indices = dict((c, i) for i, c in enumerate(words_sorted))\n",
    "    indices_word = dict((i, c) for i, c in enumerate(words_sorted))\n",
    "    corpus = np.array([ word_indices[w] for w in words ])\n",
    "\n",
    "    return corpus, word_indices, indices_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_ids, word_ids, ids_word = preprocess('you say goodbye and i say hello.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Co-occurence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_co_matrix(sent_ids, vocabulary_size, window_size=1):\n",
    "    co_matrix = np.zeros((vocabulary_size, vocabulary_size))\n",
    "    M = len(sent_ids)\n",
    "\n",
    "    for idx, word_id in enumerate(sent_ids):\n",
    "        for i in range(1, window_size+1):\n",
    "            left_idx = idx - i\n",
    "            right_idx = idx + i\n",
    "\n",
    "            if left_idx >= 0:\n",
    "                left_word_id = sent_ids[left_idx]\n",
    "                co_matrix[word_id, left_word_id] += 1\n",
    "\n",
    "            if right_idx <= M - 1:\n",
    "                right_word_id = sent_ids[right_idx]\n",
    "                co_matrix[word_id, right_word_id] += 1\n",
    "                \n",
    "    return co_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = create_co_matrix(sent_ids, len(set(sent_ids)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_similarity(x, y):\n",
    "    eps = 1e-8\n",
    "    nx = x / np.sqrt(np.sum(x**2) +eps)\n",
    "    ny = y / np.sqrt(np.sum(y**2) +eps)\n",
    "    \n",
    "    return nx@ny\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7071067758832467"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "w1_v = C[word_ids['you']]\n",
    "w2_v = C[word_ids['i']]\n",
    "\n",
    "cos_similarity(w1_v, w2_v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(word, word_ids, ids_word, co_matrix, N=5):\n",
    "    if word not in word_ids.keys():\n",
    "        print(f'{word} doesn\\'t exit!')\n",
    "        return\n",
    "    \n",
    "    word_v = co_matrix[word_ids[word]]\n",
    "    similarity = np.array([cos_similarity(word_v, co_matrix[w_id]) for (w_id, w) in ids_word.items()])\n",
    "\n",
    "    sorted_index = (-1 * similarity).argsort()\n",
    "    names = [ids_word[i] for i in sorted_index[:N]]\n",
    "    prob = [(n, s) for n, s in zip(names, similarity[sorted_index])] \n",
    "    return prob\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('you', 0.9999999900000001),\n",
       " ('goodbye', 0.7071067758832467),\n",
       " ('hello', 0.7071067758832467),\n",
       " ('i', 0.7071067758832467),\n",
       " ('.', 0.0)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('you', word_ids, ids_word, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PMI(C, eps=1e-8):\n",
    "    pmi = np.zeros_like(C)\n",
    "\n",
    "    total_freqs = np.sum(C) # total occurances of words\n",
    "    word_freq = np.sum(C, axis=1)\n",
    "    \n",
    "    word_len, context_len = C.shape\n",
    "    for i in range(word_len):\n",
    "        for j in range(context_len):\n",
    "            pmi_x = np.log2(C[i, j]*total_freqs /(word_freq[i]*word_freq[j]) + eps)\n",
    "            pmi[i, j] = max(0, pmi_x)\n",
    "\n",
    "    return pmi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppmi = PMI(C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 1., 1., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 2.80735492, 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.        , 1.80735493, 0.        , 1.80735493,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 1.80735493, 0.        , 0.        , 0.        ,\n",
       "        0.80735493, 0.        ],\n",
       "       [2.80735492, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.80735493, 0.        ],\n",
       "       [0.        , 1.80735493, 0.        , 0.        , 0.        ,\n",
       "        0.80735493, 0.        ],\n",
       "       [0.        , 0.        , 0.80735493, 0.80735493, 0.80735493,\n",
       "        0.        , 1.80735493],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.80735493, 0.        ]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppmi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, Vt = np.linalg.svd(ppmi)\n",
    "\n",
    "U_trunc = -1 * U[:, :2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7faab3e8c2e0>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for word, word_id in word_ids.items():\n",
    "    plt.annotate(word, (U_trunc[word_id, 0], U_trunc[word_id, 1]))\n",
    "\n",
    "plt.scatter(U_trunc[:,0], U_trunc[:, 1], alpha=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return max(0, x)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    if x.dim == 1: # for a single point\n",
    "        x = x - np.max(x)\n",
    "        return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "    if x.dim == 2: # for mini-batch\n",
    "        x = x - np.max(x, axis=1, keepdims=True)\n",
    "        x = np.exp(x)\n",
    "        return x / np.sum(x, axis=1, keepdims=True)\n",
    "\n",
    "    \n",
    "def cross_entropy(y, t): # batch_N * |V|\n",
    "\n",
    "    if y.ndim == 1: # for a single point\n",
    "        t = t.reshape(1, t.size)  # 1 * |V|\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    if t.size == y.size: # if t is one-hot, convert to 'id'\n",
    "        t = t.argmax(axis=1)\n",
    "\n",
    "    batch_size = len(y)\n",
    "    \n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "\n",
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, z):\n",
    "        self.y = softmax(z)\n",
    "        return self.y\n",
    "\n",
    "    def backward(self, dout):\n",
    "        pass\n",
    "#         return self.y + self.y * dout\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, z):\n",
    "        y = sigmoid(z)\n",
    "        self.y = y\n",
    "        return y\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return self.y * (1 - self.y) * dout\n",
    "\n",
    "\n",
    "class SigmoidWithLoss:\n",
    "    '''\n",
    "    binary classification\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "\n",
    "    def forward(self, z, t): \n",
    "        self.y = sigmoid(z) # # batch_size * 1\n",
    "        self.t = t # batch_size * 1 or batch_size * 2(ont-hot)\n",
    "        loss= cross_entropy(np.c_[1-self.y, self.y], t)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = len(self.t)\n",
    "        return (self.y - self.t) * dout / batch_size\n",
    "\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "\n",
    "    def forward(self, z, t): \n",
    "        self.y = softmax(z) # shape: batch_N * |V|\n",
    "        self.t = t # batch_size * 1 or batch_size * |V|(ont-hot)\n",
    "\n",
    "        if self.t.size == self.y.size: # batch_size * 1\n",
    "            self.t = self.t.argmax(axis=1)\n",
    "\n",
    "        loss= cross_entropy(self.y, self.t)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        '''\n",
    "        (y - t)/batch_size\n",
    "        '''\n",
    "        batch_size = len(self.t)\n",
    "        dx = self.y.copy()\n",
    "        dx[np.arange(batch_size), self.t]-=1\n",
    "\n",
    "        return dx * dout / batch_size # batch_size * |V|\n",
    "\n",
    "\n",
    "class MatrixMul:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.x = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        W, = self.params\n",
    "        out = x @ W\n",
    "        self.x = x\n",
    "        return out\n",
    "\n",
    "    def backward(self, out):\n",
    "        W, = self.params\n",
    "        dx = out @ W.T\n",
    "        dW = self.x.T @ out\n",
    "        self.grads[0][...] = dw\n",
    "        return dx\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "class Affine:\n",
    "    '''\n",
    "    density layer, fully connected layer\n",
    "    '''\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "        self.grads = [np.zero_like(W), np.zero_like(b)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        W, b = self.params\n",
    "        z = x @ W + b\n",
    "        self.x = x \n",
    "        return z\n",
    "\n",
    "    def backward(self, out):\n",
    "        W, b = self.params\n",
    "      \n",
    "        dx = out @ W.T\n",
    "        dW = self.x.T @ out\n",
    "        db = np.sum(out, axis=0)\n",
    "\n",
    "        self.grads[0][...] = dW\n",
    "        self.grads[1][...] = db\n",
    "        \n",
    "        return dx\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, out_size):\n",
    "\n",
    "        I, H, O = input_size, hidden_size, out_size\n",
    "        \n",
    "        W1 = np.random.rand(I, H)\n",
    "        b1 = np.random.rand(H)\n",
    "        W2 = np.random.rand(H, O)\n",
    "        b2 = np.random.rand(O)\n",
    "    \n",
    "        self.layers = [ Affine(W1, b1), Sigmoid(), Affine(W2, b2)]\n",
    "\n",
    "        self.params = []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.06144962, 0.9572717 , 0.82174549, 0.86764326, 0.71409764,\n",
       "       0.88015585, 0.8821359 ])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inference(contexts):\n",
    "    '''\n",
    "    windsize= 1\n",
    "    '''\n",
    "    c1, c2 = contexts\n",
    "   \n",
    "    D = len(c1)\n",
    "    H = 3\n",
    "\n",
    "    W_in = np.random.rand(D, H)\n",
    "    W_out = np.random.rand(H, D)\n",
    "    \n",
    "    in_layer = MatrixMul(W_in)\n",
    "    out_layer = MatrixMul(W_out)\n",
    "    \n",
    "    h1=in_layer.forward(c1)\n",
    "    h2=in_layer.forward(c2)\n",
    "    h = (h1 + h2) / len(contexts)\n",
    "    s = out_layer.forward(h)\n",
    "   \n",
    "    return s\n",
    "    \n",
    "inference([[1, 0, 0,0,0,0,0,], [0,0,0,1,0,0,0,]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## construct contexts and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_context_targets(corpus, window_size = 1):\n",
    "    N = len(corpus)\n",
    "    target = corpus[window_size:N-window_size]\n",
    " \n",
    "    contexts = []\n",
    "    for idx, w_id in enumerate(corpus[window_size:N-window_size]):\n",
    "        context_w = []\n",
    "\n",
    "        for i in range(1, window_size+1):\n",
    "            left_idx = idx+window_size-i\n",
    "            right_idx = idx+window_size+i\n",
    "\n",
    "            left_id = corpus[left_idx]\n",
    "            right_id = corpus[right_idx]\n",
    "            \n",
    "            context_w+=[left_id, right_id]\n",
    "\n",
    "        contexts.append(context_w)\n",
    "\n",
    "    return contexts, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[0, 2], [1, 3], [2, 4], [3, 1], [4, 5], [1, 6]], [1, 2, 3, 4, 1, 5])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts, target = create_context_targets([0, 1, 2, 3, 4, 1, 5, 6])\n",
    "\n",
    "contexts, target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_onehot(corpus, voc_size):\n",
    "    '''\n",
    "    convert the list of ids of a sentence to one-hot representation\n",
    "    [0, 1, 2, 1, 0] => [[1,0,0], [0, 1, 0], [0, 0, 1], [0, 1, 0], [1,0,0]]\n",
    "    '''\n",
    "\n",
    "    N = corpus.shape[0] # the numer of words in a sentence or sentences\n",
    "\n",
    "    if corpus.ndim == 1: # the numer of words in a sentence\n",
    "        one_hot = np.zeros((N, voc_size))\n",
    "        for idx, ids in enumerate(corpus):\n",
    "            one_hot[idx, ids] = 1\n",
    "    \n",
    "    if corpus.ndim == 2: # the numer of sentences\n",
    "        C = corpus.shape[1] # the numer of words in a sentence\n",
    "        one_hot = np.zeros((N, C, voc_size))\n",
    "\n",
    "        for idx, sent_ids in enumerate(corpus):\n",
    "            for (w_idx, w_id) in enumerate(sent_ids):\n",
    "                one_hot[idx, w_idx, w_id] = 1\n",
    "\n",
    "    return  one_hot\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6, 7),\n",
       " array([[0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0.]]))"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "target_onehot = convert_onehot(np.array(target), len(word_ids))\n",
    "\n",
    "target_onehot.shape, target_onehot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.]]),\n",
       " (6, 2, 7))"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "contexts_onehot=convert_onehot(np.array(contexts), len(word_ids))\n",
    "\n",
    "contexts_onehot[0], contexts_onehot.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCBOW:\n",
    "    def __init__(self, voca_size, hidden_size):\n",
    "        W_in = np.random.randn(voca_size, hidden_size)\n",
    "        W_out = np.random.randn(hidden_size, voca_size)\n",
    "        \n",
    "        self.in_layer0=MatrixMul(W_in)\n",
    "        self.in_layer1=MatrixMul(W_in)\n",
    "        self.out_layer=MatrixMul(W_out)\n",
    "        self.loss_layer=SoftmaxWithLoss()\n",
    "\n",
    "    def forward(self, contexts, target):\n",
    "        h0 = self.in_layer0.forward(contexts[:, 0])\n",
    "        h1 = self.in_layer1.forward(contexts[:, 1])\n",
    "        h = (h0+h1)*0.5 # batch_N * D\n",
    "        score=self.out_layer.forward(h) # batch_N * |V|\n",
    "        loss=self.loss_layer.forward(score, target)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        do=self.loss_layer.backward(dout)\n",
    "        do = self.out_layer.backward(do)\n",
    "        do*=0.5\n",
    "        self.in_layer0.backward(do)\n",
    "        self.in_layer1.backward(do)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 3\n",
    "window_size = 1\n",
    "batch_size = 3\n",
    "max_epochs = 100\n",
    "\n",
    "sent_ids, word_ids, ids_word = preprocess('you say goodbye and i say hello.')\n",
    "contexts, target = create_context_targets(sent_ids, window_size)\n",
    "voc_size = len(word_ids)\n",
    "contexts_onehot=convert_onehot(np.array(contexts), voc_size)\n",
    "\n",
    "model=SimpleCBOW(voc_size, hidden_size)\n",
    "for epoch in max_epochs:\n",
    "    loss=0\n",
    "    for i in batch_iters:\n",
    "        inputs, target = batch_data\n",
    "        loss+=model.forward(inputs, target)\n",
    "        model.backward()\n",
    "        optimizer.update()\n",
    "    print(f'epoch {epoch}: loss {loss}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.idx = None\n",
    "\n",
    "    def forward(self, idx):\n",
    "        '''\n",
    "        forward([0, 2, 0, 4])\n",
    "        '''\n",
    "        W, = self.params\n",
    "        out = W[idx]\n",
    "        self.idx = idx\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dW, = self.grads\n",
    "     \n",
    "        for i, word_id in enumerate(self.idx):\n",
    "            dW[word_id] += dout[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NegativeSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "h_t = tanh(h_{t-1} W_h + x_t W_x + b)\n",
    "$$\n",
    "\n",
    "\n",
    "- batch_size: batch_N\n",
    "\n",
    "- input dimension: D\n",
    "\n",
    "- hidden_state dimension: H\n",
    "\n",
    "\n",
    "$$\n",
    "shape(h_t) = (batch\\_N, H) * (H,H) + (batch\\_N, D)*(D, H) + (1, H) = (batch\\_N, H)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, W_h, W_x, b):\n",
    "        self.params = [W_h, W_x, b]\n",
    "        self.grads = [np.zeros_like(W_h), np.zeros_like(W_x), np.zeros_like(b)]\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, x, h_prev):\n",
    "        '''\n",
    "        x: (batch_N, D)\n",
    "        h_prev: (batch_N, H)\n",
    "        @return\n",
    "            h_next: (batch_N, H)\n",
    "        '''\n",
    "        W_h, W_x, b = self.params\n",
    "        h_next = h_prev @ W_h + x@W_x + b\n",
    "        h_next = np.tanh(h_next)\n",
    "\n",
    "        self.cache=(x, h_prev, h_next)\n",
    "\n",
    "        return h_next\n",
    "\n",
    "    def backward(self, dout):\n",
    "        x, h_prev, h_next = self.cache\n",
    "        W_h, W_x, b = self.params\n",
    "\n",
    "        do = dout*(1-h_next**2)\n",
    "        db = np.sum(do, axis=0)\n",
    "        dx = do @ W_x.T\n",
    "        dh_prev=do @ W_h.T\n",
    "        dwx = x.T@do\n",
    "        dwh=h_prev.T@do\n",
    "        \n",
    "        self.grad[0][...]=dwh\n",
    "        self.grad[1][...]=dwx\n",
    "        self.grad[2][...]=b\n",
    "\n",
    "        return dx, dh_prev\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeRNN:\n",
    "    def __init__(self, W_h, W_x, b, stateful=False):\n",
    "        self.params = [W_h, W_x, b]\n",
    "        self.grads = [np.zeros_like(W_h), np.zeros_like(W_x), np.zeros_like(b)]\n",
    "        \n",
    "        self.layers = []\n",
    "        self.stateful=stateful\n",
    "        self.h = None\n",
    "        self.dh = None\n",
    "\n",
    "    def set_state(self, prev_state):\n",
    "        self.h = prev_state\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.h = None\n",
    "        \n",
    "\n",
    "    def forward(self, xs):\n",
    "        '''\n",
    "        xs: (batch_N, seq_t, D)\n",
    "\n",
    "        @return\n",
    "            hs_next: (batch_N, seq_t, H)\n",
    "        '''\n",
    "\n",
    "        W_h, W_x, b = self.params\n",
    "        batch_N, seq_t, D = xs.shape\n",
    "\n",
    "        hs = np.empty((batch_N, seq_t, H))\n",
    "\n",
    "        if self.h is None or not self.stateful:\n",
    "            self.h=np.zeros((batch_N, H))\n",
    "        \n",
    "        for i in range(seq_t):\n",
    "            layer_i = RNN(*self.params)\n",
    "            self.h = layer_i.forward(xs[:, i, :], self.h)\n",
    "            hs[:, i, :] = self.h\n",
    "            self.layers.append(layer_i)\n",
    "\n",
    "        return hs\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        batch_N, seq_t, H = dhs.shape\n",
    "        W_h, W_x, b = self.params\n",
    "        D, H = W_x.shape\n",
    "\n",
    "        dh = 0\n",
    "        grads = [0, 0, 0]\n",
    "        dxs = np.empty(( batch_N, seq_t, D))\n",
    "        for t in reversed(range(seq_t)):\n",
    "            layer = self.layers[t]\n",
    "            dx, dh = layer.backward(dhs[:, t, :]+dh) # (batch_N, H)\n",
    "            dxs[:, t, :] = dx\n",
    "\n",
    "            # all rnn layers share the same W_h, W_x, b\n",
    "            for i, grad in enumerate(layer.grads):\n",
    "                grads[i] += grad\n",
    "\n",
    "        for i, grad in enumerate(grads):\n",
    "            self.grads[i][...] = grad\n",
    "\n",
    "        self.dh = dh\n",
    "\n",
    "        return dxs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEmbedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "\n",
    "        self.layers = []\n",
    "        \n",
    "    def forward(self, xs):\n",
    "        '''\n",
    "        xs: (batch_N, seq_t, 1)\n",
    "        [ [[1], \n",
    "           [0], \n",
    "           [2]\n",
    "          ],\n",
    "          [[0],\n",
    "           [4],\n",
    "           [2]\n",
    "          ]\n",
    "        ]\n",
    "        @return\n",
    "            word_vector: (batch_N, seq_t, D)\n",
    "        '''\n",
    "        batch_N, seq_t, _ = xs\n",
    "\n",
    "        W, = self.params\n",
    "        _, D = W.shape\n",
    "        word_s = np.empty((batch_N, seq_t, D))\n",
    "        for i in range(seq_t):\n",
    "            layer_i = Embedding(*self.params)\n",
    "            self.word = layer_i.forward(xs[:, i, :])\n",
    "            word_s[:, i, :] = self.word\n",
    "            self.layers.append(layer_i)\n",
    "       \n",
    "        return word_s\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        batch_N, seq_t, D = dout\n",
    "        grad = 0\n",
    "       \n",
    "        for t in reversed(range(seq_t)): # there's no need to reverse the sequence\n",
    "            layer = self.layers[t]\n",
    "            layer.backward(dout[:, t, :])\n",
    "            grad += layer.grads[0]\n",
    "        \n",
    "        self.grads[0][...] = grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
