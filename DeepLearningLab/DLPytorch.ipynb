{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://atcold.github.io/pytorch-Deep-Learning/\n",
    "# https://d2l.ai/\n",
    "# http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Intro to Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabeluar data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wine = pd.read_csv('./data/winequality-white.csv', sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.0              0.27         0.36            20.7      0.045   \n",
       "1            6.3              0.30         0.34             1.6      0.049   \n",
       "2            8.1              0.28         0.40             6.9      0.050   \n",
       "3            7.2              0.23         0.32             8.5      0.058   \n",
       "4            7.2              0.23         0.32             8.5      0.058   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 45.0                 170.0   1.0010  3.00       0.45   \n",
       "1                 14.0                 132.0   0.9940  3.30       0.49   \n",
       "2                 30.0                  97.0   0.9951  3.26       0.44   \n",
       "3                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "4                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      8.8        6  \n",
       "1      9.5        6  \n",
       "2     10.1        6  \n",
       "3      9.9        6  \n",
       "4      9.9        6  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wine.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n",
       "       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n",
       "       'pH', 'sulphates', 'alcohol', 'quality'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wine.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4898, 12)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wine.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = torch.from_numpy(df_wine.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4898, 12]), 'torch.DoubleTensor')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine.size(), wine.type()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4898, 11]),\n",
       " torch.Size([4898]),\n",
       " tensor([6., 6., 6., 6., 6.], dtype=torch.float64))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = wine[:, :-1]\n",
    "target = wine[:, -1]\n",
    "\n",
    "data.size(), target.size(), target[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6., 6., 6.,  ..., 6., 7., 6.], dtype=torch.float64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_onehot = torch.zeros(target.shape[0], 10)\n",
    "\n",
    "target_onehot.scatter_(1, target.unsqueeze(1).long(), 1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_onehot[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.8548e+00, 2.7824e-01, 3.3419e-01, 6.3914e+00, 4.5772e-02, 3.5308e+01,\n",
       "        1.3836e+02, 9.9403e-01, 3.1883e+00, 4.8985e-01, 1.0514e+01],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(data, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7.1211e-01, 1.0160e-02, 1.4646e-02, 2.5726e+01, 4.7733e-04, 2.8924e+02,\n",
       "        1.8061e+03, 8.9455e-06, 2.2801e-02, 1.3025e-02, 1.5144e+00],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.var(data, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_wine = target.le(3)\n",
    "mid_wine = target.gt(3) & target.le(7)\n",
    "good_wine = target.gt(7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fixed acidity 7.6 6.858375904640272 6.678333333333333\n",
      "volatile acidity 0.33325000000000005 0.27801724137931033 0.2779722222222222\n",
      "citric acid 0.336 0.334414644529587 0.32816666666666666\n",
      "residual sugar 6.3925 6.420647083865475 5.628333333333334\n",
      "chlorides 0.0543 0.04603341847594722 0.03801111111111112\n",
      "free sulfur dioxide 53.325 35.180821626223924 36.62777777777778\n",
      "total sulfur dioxide 170.6 138.7014687100894 125.88333333333334\n",
      "density 0.9948840000000001 0.9940931928480202 0.9922143888888889\n",
      "pH 3.1875 3.1870093656875267 3.2211666666666665\n",
      "sulphates 0.4744999999999999 0.4900723712217964 0.48566666666666675\n",
      "alcohol 10.345 10.471430395913154 11.651111111111113\n"
     ]
    }
   ],
   "source": [
    "bad_mean = data[bad_wine].mean(dim=0)\n",
    "mid_mean = data[mid_wine].mean(dim=0)\n",
    "good_mean = data[good_wine].mean(dim=0)\n",
    "\n",
    "col_names = df_wine.columns[df_wine.columns!='quality']\n",
    "for name, bad_v, mid_v, good_v in zip(col_names, bad_mean, mid_mean, good_mean):\n",
    "    print(name, bad_v.item(), mid_v.item(), good_v.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2727), tensor(3258))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are 500 more good wines than your threshold predicted, so total sulfur dioxide \n",
    "predicted_index = (data[:, 6] < 141.83)\n",
    "actual_index = target > 5\n",
    "\n",
    "predicted_index.sum(), actual_index.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_matches = (predicted_index & actual_index).sum().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2018, 0.74000733406674, 0.6193984039287906)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_matches, n_matches / predicted_index.sum().item(), n_matches / actual_index.sum().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://www.gutenberg.org/files/1342/1342-0.txt -P data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('./data/1342-0.txt', encoding='utf8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "corpus = list(filter(lambda x: len(x) > 0, text.split('\\n')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    punctuation = '.,;:\"!?”“_-'\n",
    "\n",
    "    words = text.lower().strip().replace('\\n', ' ').split()\n",
    "    \n",
    "    return [ w.strip(punctuation) for w in words ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character-level one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('impossiblemrbennetimpossiblewheniamnotacquaintedwithhim', 55)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line = corpus[200].strip()\n",
    "\n",
    "line = '“Impossible, Mr. Bennet, impossible, when I am not acquainted with him'\n",
    "\n",
    "cleaned_line = ''.join(clean_text(line))\n",
    "\n",
    "cleaned_line, len(cleaned_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([55, 128])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letter_tensor = torch.zeros(len(cleaned_line), 128)\n",
    "\n",
    "letter_tensor.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, letter in enumerate(cleaned_line):\n",
    "    idx = ord(letter) if ord(letter) < 128 else 0\n",
    "    letter_tensor[i][idx] = 1\n",
    "\n",
    "letter_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word-level one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7265, 3924)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = clean_text(text)\n",
    "\n",
    "vocabulary = set(words)\n",
    "\n",
    "word2idx = {w: idx for idx, w in enumerate(vocabulary)}\n",
    "\n",
    "len(vocabulary), word2idx['impossible']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3924 impossible\n",
      "1 624 mr\n",
      "2 2439 bennet\n",
      "3 3924 impossible\n",
      "4 1802 when\n",
      "5 1142 i\n",
      "6 6122 am\n",
      "7 4968 not\n",
      "8 2580 acquainted\n",
      "9 4226 with\n",
      "10 923 him\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 7265])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_in_line = clean_text(line)\n",
    "\n",
    "word_vector = torch.zeros(len(words_in_line), len(vocabulary))\n",
    "\n",
    "for idx, w in enumerate(words_in_line):\n",
    "    j = word2idx[w]\n",
    "    word_vector[idx][j] = 1\n",
    "    print(idx, j, w)\n",
    "\n",
    "word_vector.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Linear Regression Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "y = [0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0]\n",
    "x = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
    "\n",
    "x= torch.tensor(x)\n",
    "y = torch.tensor(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(w, b, x):\n",
    "    return w * x + b\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    ''' means squared error'''\n",
    "    return ((y_true - y_pred)**2).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1763.8848)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.ones(1)\n",
    "b = torch.zeros(1)\n",
    "\n",
    "y_pred = model(w, b, x)\n",
    "\n",
    "loss_fn(y, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_dw(y_true, y_pred):\n",
    "    return y_pred - y_true\n",
    "\n",
    "def model_dw(x):\n",
    "    return x\n",
    "\n",
    "def model_db():\n",
    "    return 1\n",
    "\n",
    "def grad_w(x, y_true,  y_pred):\n",
    "    return ((y_pred - y_true) * x).mean()\n",
    "\n",
    "def grad_b(y_true,  y_pred):\n",
    "    return (y_pred - y_true).mean()\n",
    "\n",
    "def grad_param(x, y_true, y_pred):\n",
    "    dw = grad_w(x, y_true,  y_pred)\n",
    "    db = grad_b(y_true,  y_pred)\n",
    "\n",
    "    return torch.stack([dw, db])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 loss:  80.36434173583984\n",
      "[w, b]: [1.3880701 0.0532   ]\n",
      "epoch:  1000 loss:  7.846929550170898\n",
      "[w, b]: [ 4.044966 -9.816922]\n",
      "epoch:  2000 loss:  3.8267829418182373\n",
      "[w, b]: [  4.802209 -14.103535]\n",
      "epoch:  3000 loss:  3.091984748840332\n",
      "[w, b]: [  5.12595  -15.936173]\n",
      "epoch:  4000 loss:  2.9576845169067383\n",
      "[w, b]: [  5.264355 -16.719664]\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01\n",
    "epoches = 5000\n",
    "\n",
    "def training(x, y, params, epoches, lr):\n",
    "    for epoch in range(epoches):\n",
    "        w, b = params\n",
    "\n",
    "        y_pred = model(w, b, x)\n",
    "        loss = loss_fn(y, y_pred)\n",
    "        grad = grad_param(x, y, y_pred)\n",
    "\n",
    "        params = params - lr * grad\n",
    "        \n",
    "        if epoch % 1000 == 0:\n",
    "            print('epoch: ', epoch, 'loss: ', loss.item())\n",
    "            print('[w, b]:', params.numpy())\n",
    "    return params\n",
    "\n",
    "params = training(x*0.1, y, torch.tensor([1., 0.]), epoches, lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  5.3235, -17.0544])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.9504, 12.7039, 13.9283, 26.5450, 12.9168,  8.9775,  0.9922, -5.4492,\n",
       "         8.7113, 15.0995, 19.3583])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model(*params, x*0.1)\n",
    "\n",
    "y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fdb2d2e1220>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x.numpy(), y_pred.numpy())\n",
    "plt.scatter(x.numpy(), y.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. MNIST Classifier Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_images = pd.read_csv('./data/train.csv')\n",
    "\n",
    "df_images.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 785)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_images.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fdb2d462520>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(df_images.iloc[0, 1:].values.reshape(28, 28), cmap=plt.get_cmap('gray'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    df_images = pd.read_csv(filename)\n",
    "\n",
    "    X = df_images.drop('label', axis=1)\n",
    "    y = df_images['label']\n",
    "\n",
    "    X = X.values.reshape(len(X), 1, 28, 28)\n",
    "    y = y.values\n",
    "    \n",
    "    X = torch.tensor(X).float()\n",
    "    y = torch.tensor(y)\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MNISTClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(784, 392)\n",
    "        self.fc2 = nn.Linear(392, 196)\n",
    "        self.fc3 = nn.Linear(196, 98)\n",
    "        self.fc4 = nn.Linear(98, 10)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=.2)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = inputs.view(inputs.shape[0], -1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "\n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "        \n",
    "        return x        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MNISTClassifier(\n",
       "  (fc1): Linear(in_features=784, out_features=392, bias=True)\n",
       "  (fc2): Linear(in_features=392, out_features=196, bias=True)\n",
       "  (fc3): Linear(in_features=196, out_features=98, bias=True)\n",
       "  (fc4): Linear(in_features=98, out_features=10, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MNISTClassifier()\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(X, y, model, epoches, loss_fn, optimizer):\n",
    "    for epoch in range(epoches):\n",
    "        y_pred = model(X)\n",
    "        loss_train = loss_fn(y_pred, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print('Epoch [%d/%d]: Loss: %.4f' % (epoch+1, epoches, loss_train.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]: Loss: 4.7718\n",
      "Epoch [11/50]: Loss: 0.7783\n",
      "Epoch [21/50]: Loss: 0.2212\n",
      "Epoch [31/50]: Loss: 0.0518\n",
      "Epoch [41/50]: Loss: 0.0126\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = load_data('./data/train.csv')\n",
    "\n",
    "training(\n",
    "    X_train, y_train, \n",
    "    model=model, \n",
    "    epoches=50, \n",
    "    loss_fn=loss_fn, \n",
    "    optimizer=optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = load_data('./data/test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.885"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred = model(X_test)\n",
    "\n",
    "_, y_test_pred_idx = torch.max(y_test_pred, dim=1)\n",
    "\n",
    "len([ 1 for y_pred, y_true in zip(y_test_pred_idx, y_test) if y_pred == y_true ]) / len(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. CBOW Pytorch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"How that personage haunted my dreams, I need scarcely tell you. On\n",
    "stormy nights, when the wind shook the four corners of the house and\n",
    "the surf roared along the cove and up the cliffs, I would see him in a\n",
    "thousand forms, and with a thousand diabolical expressions. Now the leg\n",
    "would be cut off at the knee, now at the hip, now he was a monstrous\n",
    "kind of a creature who had never had but the one leg, and that in the\n",
    "middle of his body. To see him leap and run and pursue me over hedge and\n",
    "ditch was the worst of nightmares. And altogether I paid pretty dear for\n",
    "my monthly fourpenny piece, in the shape of these abominable fancies\"\"\"\n",
    "\n",
    "tokens = text.replace(',','').replace('.','').lower().split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_word_id_mapping(corpus):\n",
    "    vocabulary = set(corpus)\n",
    "    voc_size = len(vocabulary)\n",
    "    \n",
    "    word_dict = {}\n",
    "    inverse_word_dict = {}\n",
    "\n",
    "    for i, word in enumerate(vocabulary):\n",
    "        word_dict[word] = i\n",
    "        inverse_word_dict[i] = word\n",
    "\n",
    "    return voc_size, word_dict, inverse_word_dict\n",
    "\n",
    "\n",
    "def get_context_idx(context, word_dict):\n",
    "    '''\n",
    "    context: ['a', 'b', 'c']\n",
    "    '''\n",
    "    idx = [ word_dict[w] for w in context ]\n",
    "\n",
    "    return torch.tensor(idx, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context_target(text, win_size = 2):\n",
    "    data = []\n",
    "\n",
    "    for i in range(win_size, len(text) - win_size):\n",
    "        context = [\n",
    "            text[i-2], text[i-1],\n",
    "            text[i+1], text[i+2],\n",
    "        ]\n",
    "        \n",
    "        target=text[i]\n",
    "    \n",
    "        data.append((context, target))\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBow(nn.Module):\n",
    "    def __init__(self, v_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(v_size, embedding_size)\n",
    "        self.fc1 = nn.Linear(embedding_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, v_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embedding = sum(self.embedding(inputs)).view(1, -1) # (1, E)\n",
    "        out = self.fc1(embedding)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.softmax(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def training(model, data, v_size, word_dict):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    loss_fn = nn.NLLLoss()\n",
    "    epoches = 100\n",
    "\n",
    "    for epoch in range(epoches):\n",
    "        epoch_loss = 0\n",
    "        for context, target in data:\n",
    "            context_ids = get_context_idx(context, word_dict)\n",
    "\n",
    "            log_probs = model(context_ids)\n",
    "            loss = loss_fn(log_probs, torch.tensor([word_dict[target]], dtype=torch.long))\n",
    "\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        if epoch == 0 or (epoch + 1) % 10 == 0:\n",
    "            print('Epoch %d/%d, Train Loss: %4.f' % (epoch + 1, epoches, epoch_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss:  532\n",
      "Epoch 10/100, Train Loss:  176\n",
      "Epoch 20/100, Train Loss:   26\n",
      "Epoch 30/100, Train Loss:    9\n",
      "Epoch 40/100, Train Loss:    5\n",
      "Epoch 50/100, Train Loss:    3\n",
      "Epoch 60/100, Train Loss:    3\n",
      "Epoch 70/100, Train Loss:    2\n",
      "Epoch 80/100, Train Loss:    2\n",
      "Epoch 90/100, Train Loss:    1\n",
      "Epoch 100/100, Train Loss:    1\n"
     ]
    }
   ],
   "source": [
    "v_size, word_dict, inverse_word_dict = build_word_id_mapping(tokens)\n",
    "\n",
    "data = build_context_target(tokens)\n",
    "\n",
    "model = CBow(v_size, 20, 64)\n",
    "\n",
    "training(model, data, v_size, word_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4131,  1.7616, -0.6311,  1.3714,  0.8231, -0.3679, -0.3469,  0.4156,\n",
       "         -1.1271,  1.6484, -0.0486, -1.6873,  0.4396, -0.7184, -0.9546,  0.0995,\n",
       "         -0.7428, -0.2030,  1.2041, -0.7599]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding( torch.tensor( [ word_dict['leap'] ], dtype=torch.long ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similarity(\n",
    "    embedding,\n",
    "    word_dict, inverse_word_dict, v_size, \n",
    "    word, top_n\n",
    "):\n",
    "    v_w1 = embedding( torch.tensor( [ word_dict[word] ], dtype=torch.long ) ).detach().numpy()\n",
    "\n",
    "    word_sim = {}\n",
    "    for i in range(v_size):\n",
    "        v_w2 = embedding(torch.tensor(i)).detach().numpy()\n",
    "        theta = np.dot(v_w1, v_w2) / (np.linalg.norm(v_w1) * np.linalg.norm(v_w2))\n",
    "        word = inverse_word_dict[i]\n",
    "        word_sim[word] = theta\n",
    "\n",
    "    words_sorted = sorted(word_sim.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "    for word, sim in words_sorted[:top_n]:\n",
    "        print(word, sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretty [0.99999994]\n",
      "altogether [0.68747884]\n",
      "my [0.62062633]\n",
      "how [0.45483753]\n",
      "that [0.40294233]\n"
     ]
    }
   ],
   "source": [
    "find_similarity(\n",
    "    model.embedding, word_dict, inverse_word_dict, v_size,\n",
    "    'pretty', 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['run', 'at', 'house', 'abominable', 'but', 'corners', 'stormy', 'over', 'and', 'thousand', 'paid', 'of', 'ditch', 'pretty', 'pursue', 'fancies', 'cliffs', 'to', 'four', 'now', 'the', 'cut', 'roared', 'me', 'wind', 'need', 'creature', 'these', 'a', 'surf', 'my', 'shape', 'how', 'monthly', 'shook', 'when', 'dreams', 'that', 'cove', 'tell', 'had', 'haunted', 'kind', 'worst', 'see', 'off', 'hedge', 'monstrous', 'middle', 'scarcely', 'piece', 'i', 'his', 'on', 'who', 'for', 'be', 'up', 'him', 'hip', 'personage', 'expressions', 'in', 'leap', 'leg', 'you', 'he', 'was', 'diabolical', 'nightmares', 'nights', 'forms', 'one', 'along', 'never', 'with', 'would', 'altogether', 'fourpenny', 'dear', 'knee', 'body'])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment%20labelled%20sentences.zip -P data\n",
    "    \n",
    "!unzip './data/sentiment labelled sentences.zip' -d data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/sentiment.txt\") as f:\n",
    "    reviews = f.read()\n",
    "\n",
    "data = pd.DataFrame([ \n",
    "    review.split('\\t') for review in reviews.split('\\n') ],\n",
    "    columns=['review', 'label']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A very, very, very slow-moving, aimless movie ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not sure who was more lost - the flat characte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Attempting artiness with black &amp; white and cle...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Very little music or anything to speak of.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The best scene in the movie was when Gerardo i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review label\n",
       "0  A very, very, very slow-moving, aimless movie ...     0\n",
       "1  Not sure who was more lost - the flat characte...     0\n",
       "2  Attempting artiness with black & white and cle...     0\n",
       "3       Very little music or anything to speak of.       0\n",
       "4  The best scene in the movie was when Gerardo i...     1"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 2)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string, re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words_en = stopwords.words('english')\n",
    "\n",
    "def load_data():\n",
    "    with open(\"./data/sentiment.txt\") as f:\n",
    "        reviews = f.read()\n",
    "\n",
    "    data = pd.DataFrame([ \n",
    "      review.split('\\t') for review in reviews.split('\\n') ],\n",
    "      columns=['review', 'label']\n",
    "    )\n",
    "\n",
    "    return data\n",
    "\n",
    "def clen_sentence(text):\n",
    "    '''\n",
    "    extract useful tokens from a sentence\n",
    "    '''\n",
    "    text = text.lower()\n",
    "\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # remove stopping words\n",
    "    tokens = [ w for w in tokens if w not in stop_words_en ]\n",
    "    tokens = [ w for w in tokens if len(w) > 1 ]\n",
    "    \n",
    "    # return tokens\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def build_vocabulary(corpus):\n",
    "    '''\n",
    "    tokens: a list of tokens of the corpus\n",
    "\n",
    "    build_vocabulary(['work', 'eat', 'school', 'read', 'cat', 'eat'])\n",
    "    '''\n",
    "    \n",
    "    tokens = [ w for line in corpus for w in line ]\n",
    "    vocab = set(tokens)\n",
    "\n",
    "    word_id = {}\n",
    "    inverse_word_id = {}\n",
    "    for i, word in enumerate(vocab):\n",
    "        word_id[word] = i+1\n",
    "        inverse_word_id[i+1] = word\n",
    "\n",
    "    pad_char = '*'\n",
    "    word_id[pad_char] = 0\n",
    "    inverse_word_id[0] = pad_char\n",
    "    vocab.add(pad_char)\n",
    "\n",
    "    return vocab, word_id, inverse_word_id\n",
    "\n",
    "def pad_text(text, seq_len, pad_str):\n",
    "    '''\n",
    "    RNN takes a fixed length of sequence\n",
    "    '''\n",
    "\n",
    "    review = None\n",
    "    text_size = len(text)\n",
    "\n",
    "    if text_size >= seq_len:\n",
    "        review = text[:seq_len]\n",
    "    else:\n",
    "        review = [pad_str] * (seq_len - text_size) + text\n",
    "    \n",
    "    return review\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(data):\n",
    "    data['cleaned_review'] = [ clen_sentence(review) for review in data['review'].values ]\n",
    "\n",
    "    data['word_len'] = [ len(x.split()) for x in data['cleaned_review'] ]\n",
    "    data = data[data['word_len'] > 1]\n",
    "    \n",
    "    print('max len: ', data['word_len'].max())\n",
    "    print('min len: ', data['word_len'].min())\n",
    "    print('mean len: ', data['word_len'].mean())\n",
    "\n",
    "    corpus = [ x.split() for x in data['cleaned_review'] ]\n",
    "    label = data['label'].values.astype(int)\n",
    "    \n",
    "    return corpus, label\n",
    "\n",
    "def text_preprocess(seq_len=40):\n",
    "    df = load_data()\n",
    "    corpus, label = clean_df(df)\n",
    "    vocab, word_id, inverse_word_id = build_vocabulary(corpus)\n",
    "\n",
    "    padding_corpus = [ pad_text(text, seq_len, inverse_word_id[0]) for text in corpus ]\n",
    "    encoded_corpus = np.array([ [ word_id[t] for t in sent ] for sent in padding_corpus ])\n",
    "\n",
    "    # save\n",
    "    np.savetxt(\n",
    "        \"data/cleaned_review.csv\", \n",
    "        np.concatenate((encoded_corpus, label[:, np.newaxis]), axis=1), \n",
    "        fmt='%d',\n",
    "        delimiter=\",\"\n",
    "    )\n",
    "    \n",
    "\n",
    "    with open('data/word_id.json', 'w') as f:\n",
    "        json.dump(word_id, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import nn\n",
    "from torch import optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_data_loader(X, y, batch_size):\n",
    "    '''\n",
    "    TensorDataset is a subclass of Dataset, \n",
    "    as long as the tensor is passed in, it is indexed by the first dimension.\n",
    "    '''\n",
    "    total_len = len(X)\n",
    "    train_cutoff = int(0.7 * total_len)\n",
    "    valid_cutoff = int(0.8 * total_len)\n",
    "\n",
    "    rand_idx = torch.randperm(total_len)\n",
    "    train_index = rand_idx[:train_cutoff]\n",
    "    valid_index = rand_idx[train_cutoff:valid_cutoff]\n",
    "    test_index = rand_idx[valid_cutoff:]\n",
    "\n",
    "    \n",
    "    X_train, y_train = torch.Tensor(X[train_index]).long(), torch.Tensor(y[train_index]).float()\n",
    "    X_valid, y_valid = torch.Tensor(X[valid_index]).long(), torch.Tensor(y[valid_index]).float()\n",
    "    X_test, y_test = torch.Tensor(X[test_index]).long(), torch.Tensor(y[test_index]).float()\n",
    "    \n",
    "    train_data = TensorDataset(X_train, y_train)\n",
    "    valid_data = TensorDataset(X_valid, y_valid)\n",
    "    test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_data, batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size, shuffle=True)\n",
    "    \n",
    "    return train_loader, valid_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, n_layers, output_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size  \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers \n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embed_dim, hidden_dim, \n",
    "            n_layers, batch_first=True, \n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Embedding: (batch_size, seq_n, D)\n",
    "        embed_words = self.embedding(inputs)\n",
    "\n",
    "        # out: (batch_size, seq_n, H)\n",
    "        out, h = self.lstm(embed_words)\n",
    "        out = out.contiguous().view(-1, hidden_dim)\n",
    "  \n",
    "        # fc_out: (batch_size*seq_n, 1)\n",
    "        fc_out = self.fc(out)\n",
    "        fc_out = torch.sigmoid(fc_out)\n",
    "        # fc_out: (batch_size, seq_n)\n",
    "        fc_out = fc_out.view(embed_words.shape[0], -1) \n",
    "\n",
    "        # fc_out: (batch_size, 1)\n",
    "        sigmoid_last = fc_out[:, -2:-1].squeeze(dim=1)\n",
    "\n",
    "\n",
    "        return sigmoid_last, h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training():\n",
    "    prev_valid_loss = 99999\n",
    "\n",
    "    for epoch in range(epoches):\n",
    "        model.train()\n",
    "        for inputs, t_label in train_loader:\n",
    "            out, h = model(inputs)\n",
    "\n",
    "            # .squeeze() => dimensions of 1 removed\n",
    "            loss = loss_fn(out, t_label)\n",
    "\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "             \n",
    "        model.eval()\n",
    "        valid_loss = []\n",
    "        for inputs, v_label in valid_loader:\n",
    "            v_out, v_h = model(inputs)\n",
    "            v_loss = loss_fn(v_out, v_label)\n",
    "            valid_loss.append(v_loss.item())\n",
    "\n",
    "        mean_valid_loss = np.mean(valid_loss)\n",
    "        print('Epoch %d, train loss: %.4f, valid loss: %.4f' % (epoch, loss.item(), mean_valid_loss))\n",
    "        \n",
    "        # early stopping\n",
    "#         if mean_valid_loss > prev_valid_loss:\n",
    "#             break\n",
    "#         else:\n",
    "#             prev_valid_loss = mean_valid_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max len:  41\n",
      "min len:  2\n",
      "mean len:  6.35348997926745\n",
      "vocab size,  5251\n",
      "(2894, 40) (2894,)\n",
      "SentimentLSTM(\n",
      "  (embedding): Embedding(5251, 30)\n",
      "  (lstm): LSTM(30, 100, batch_first=True)\n",
      "  (fc): Linear(in_features=100, out_features=1, bias=True)\n",
      ")\n",
      "embedding.weight torch.Size([5251, 30])\n",
      "lstm.weight_ih_l0 torch.Size([400, 30])\n",
      "lstm.weight_hh_l0 torch.Size([400, 100])\n",
      "lstm.bias_ih_l0 torch.Size([400])\n",
      "lstm.bias_hh_l0 torch.Size([400])\n",
      "fc.weight torch.Size([1, 100])\n",
      "fc.bias torch.Size([1])\n",
      "Epoch 0, train loss: 1.2273, valid loss: 0.6587\n",
      "Epoch 1, train loss: 1.1266, valid loss: 0.6344\n",
      "Epoch 2, train loss: 0.4216, valid loss: 0.6510\n"
     ]
    }
   ],
   "source": [
    "# step 1\n",
    "text_preprocess()\n",
    "\n",
    "cleaned_df = pd.read_csv('data/cleaned_review.csv', header=None)\n",
    "with open('data/word_id.json') as f:\n",
    "    word_id = json.load(f)\n",
    "\n",
    "\n",
    "encoded_corpus = cleaned_df.iloc[:, :-1].values\n",
    "label = cleaned_df.iloc[:, -1].values\n",
    "\n",
    "print('vocab size, ', len(word_id))\n",
    "print(encoded_corpus.shape, label.shape)\n",
    "\n",
    "# step 2\n",
    "parameters = {\n",
    "  'vocab_size': len(word_id),\n",
    "  'embed_dim': 30,\n",
    "  'hidden_dim': 100,\n",
    "  'n_layers': 1,\n",
    "  'output_dim': 1\n",
    "}\n",
    "\n",
    "model = SentimentLSTM(**parameters)\n",
    "print(model)\n",
    "for n, p in model.named_parameters():\n",
    "    print(n, p.shape)\n",
    "\n",
    "# step 3\n",
    "train_loader, valid_loader, test_loader = build_data_loader(encoded_corpus, label, batch_size=1)\n",
    "\n",
    "# step 4\n",
    "epoches = 3\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "training()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6452\n",
      "Test Accuracy: 0.67\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    model = SentimentLSTM(**parameters)\n",
    "    model.load_state_dict(torch.load('model.pkl'))\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = []\n",
    "    toal_correct=0\n",
    "    for inputs, v_label in test_loader:\n",
    "        v_out, v_h = model(inputs)        \n",
    "        v_loss = loss_fn(v_out, v_label)\n",
    "        \n",
    "        test_loss.append(v_loss.item())        \n",
    "\n",
    "        pred_label = torch.round(v_out)\n",
    "        batch_correct = pred_label.eq(v_label.view_as(pred_label)).sum()\n",
    "        toal_correct += batch_correct\n",
    "        \n",
    "    print(\"Test Loss: {:.4f}\".format(np.mean(test_loss)))\n",
    "    print(\"Test Accuracy: {:.2f}\".format(toal_correct/len(test_loader.dataset)))\n",
    "\n",
    "test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "all layers have parameter 'params', method 'forward()` \n",
    "\"\"\"\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.y = 1 / (1 + np.exp(-x))\n",
    "        return self.y\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        '''\n",
    "        dy/dx = y(1-y)\n",
    "        '''\n",
    "        dx = dout * (1- self.y) * self.y\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Affine: \n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "        \n",
    "        self.grads = [\n",
    "            np.zeros_like(W),\n",
    "            np.zeros_like(b)\n",
    "        ]\n",
    "\n",
    "        self.x = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "             x @ W + b\n",
    "        (N, D) * (D, H) + (1, H)= (N, H)\n",
    "        \"\"\"\n",
    "\n",
    "        W, b = self.params\n",
    "        \n",
    "        y = np.dot(x, W) + b\n",
    "        \n",
    "        self.x = x\n",
    "\n",
    "        return y\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        W, b = self.params\n",
    "\n",
    "        dx = dout @ W.T\n",
    "        dW = self.x.T @ dout\n",
    "        db = np.sum(dout, axis=0)\n",
    "        \n",
    "        self.grsds\n",
    "        \n",
    "        return dx\n",
    "        \n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        I, H, O = input_size, hidden_size, output_size\n",
    "        \n",
    "        self.W1 = np.random.randn(I, H)\n",
    "        self.b1 = np.random.randn(1, H)\n",
    "        self.W2 = np.random.randn(H, O)\n",
    "        self.b2 = np.random.randn(1, O)\n",
    "        \n",
    "        self.layers = [\n",
    "            Affine(self.W1, self. b1),\n",
    "            Sigmoid(),\n",
    "            Affine(self.W2, self. b2)\n",
    "        ]\n",
    "        \n",
    "        self.params, self.grads = []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "            \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "             \n",
    "    def backward(self, dout=1):\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "    \n",
    "def training():\n",
    "    epoches = 30\n",
    "    batch_size = 10\n",
    "    hidden_size = 10\n",
    "    \n",
    "    model = TwoLayerNet(2, hidden_size, 3)\n",
    "\n",
    "    data_counts = len(X)\n",
    "    batches = data_counts / batch_size\n",
    "\n",
    "    for epoch in range(epoches):\n",
    "        epoche_loss = 0\n",
    "        \n",
    "        for i in range(batches):\n",
    "            batch_x = x[i * batch_size, (i+1) * batch_size]\n",
    "            batch_y = y[i * batch_size, (i+1) * batch_size]\n",
    "\n",
    "            y_pred = model.forward(batch_x)\n",
    "\n",
    "            loss = loss_fn(y_pred, batch_y)\n",
    "            model.backward()\n",
    "            optimizer.update(model.params, model.grads)\n",
    "            \n",
    "            epoche_loss += loss\n",
    "\n",
    "def RNN_training():\n",
    "    corpus_size = len(corpus)\n",
    "    vocab_size = len(set(corpus))\n",
    "\n",
    "    xs = corpus[:-1]\n",
    "    ts = corpus[1:]\n",
    "    \n",
    "    batch_N = 10\n",
    "    time_size = 5\n",
    "    \n",
    "    data_size = len(xs)\n",
    "    # the number of times we move \n",
    "    max_iters = data_size // (batch_N * time_size)\n",
    "\n",
    "\n",
    "    time_idx = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    wordvec_dim = 100\n",
    "    hidden_dim = 100\n",
    "\n",
    "    lr = 0.1\n",
    "    max_epoch = 100\n",
    "\n",
    "    '''\n",
    "    Embedding: (V, D)\n",
    "    W_x: (D, H)\n",
    "    W_h: (H, H)\n",
    "    '''\n",
    "    model = SimpleRnnlm(vocab_size, wordvec_dim, hidden_dim)\n",
    "    optimizer = SGD(lr)\n",
    "\n",
    "\n",
    "    '''\n",
    "    jump: the number of words in each batch\n",
    "    so, the 2nd batch, we should move 1*jump, \n",
    "        the 3rd batch, move 2*jump\n",
    "    '''\n",
    "    jump = data_size // batch_N \n",
    "    offsets = [i * jump for i in range(batch_N)]\n",
    "    \n",
    "\n",
    "    for epoch in range(max_epoch):\n",
    "        for iter in range(max_iters):\n",
    "            '''\n",
    "            (N, T, D)\n",
    "            '''\n",
    "            batch_x = np.empty((batch_N, time_size), dtype='i')\n",
    "            batch_t = np.empty((batch_N, time_size), dtype='i')\n",
    "            for t in range(time_size):\n",
    "                for i, offset in enumerate(offsets):\n",
    "                    batch_x[i, t] = xs[(offset + time_idx) % data_size]\n",
    "                    batch_t[i, t] = ts[(offset + time_idx) % data_size]\n",
    "                time_idx += 1\n",
    "\n",
    "            loss = model.forward(batch_x, batch_t)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
