{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Binary Logistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p(y=1|x) = \\sigma(w^Tx) = \\frac{1}{1 + e^{-w^Tx}} \\\\\n",
    " \\\\\n",
    "p(y=0|x) = 1 - p(y=1|x)\n",
    " \\\\\n",
    "$$\n",
    "\n",
    "Therefore, $y_n \\sim Bern(n, p_n)$\n",
    "\n",
    "$$\n",
    "p(y_n= y|x_n) = p_n^{y_n} (1 - p_n)^{1 - y_n} = \\sigma(w \\cdot x_n)^{y_n} (1 - \\sigma(w \\cdot x_n))^{1 - y_n}\n",
    "$$\n",
    "\n",
    "The negative log-likelihood function is\n",
    "\n",
    "$$\n",
    "NLL(\\theta) = -\\sum^N  y_n log(p(x_n)) - (1 - y_n)log(1 - p(x_n)) \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "class BinaryLogistic:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y, ln=0.001, iters=10):\n",
    "        N, D = X.shape\n",
    "\n",
    "        theta_hat = np.random.randn(D)\n",
    "        for i in range(iters):\n",
    "            print('BCE', self.crossEntropyLoss(X, y, theta_hat))\n",
    "       \n",
    "            p = logistic(X @ theta_hat)\n",
    "            grad = X.T @ (p - y)\n",
    "            theta_hat -= ln * grad\n",
    "\n",
    "        self.theta_hat = theta_hat\n",
    "\n",
    "    def crossEntropyLoss(self, X, y, theta):\n",
    "        \"\"\"\n",
    "        Negative Loss function\n",
    "        \"\"\"\n",
    "        epsilon = 1e-12\n",
    "\n",
    "        p = logistic(X @ theta)\n",
    "\n",
    "        cost = -np.average(y * np.log(p + epsilon) + (1 - y) * np.log(1 - p + epsilon))\n",
    "\n",
    "        return cost\n",
    "\n",
    "    \n",
    "    def prob(self, X_test):\n",
    "        self.p = logistic(X_test @ self.theta_hat)\n",
    "        return self.p.round()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wuxiaopan/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:68: FutureWarning: Pass n_class=2 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((259, 65), (259,), (101, 65))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits = datasets.load_digits(2) # binary classificatin (0 or 1)\n",
    "\n",
    "X, y = digits.data, digits.target\n",
    "X = np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.28)\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCE 14.901754874815197\n",
      "BCE 13.868852297569818\n",
      "BCE 6.957870844883697\n",
      "BCE 1.5087469923664973\n",
      "BCE 0.18503044234062058\n",
      "BCE 0.15197248972137015\n",
      "BCE 0.1195621870020534\n",
      "BCE 0.11547517390019794\n",
      "BCE 0.11331837736727626\n",
      "BCE 0.11203724941947998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 2.48951843e-01,  2.17734610e+00,  8.06015587e-01, -1.03437999e+00,\n",
       "       -9.05772856e-02,  2.03101491e+00, -9.32049383e-02,  1.37344358e+00,\n",
       "       -1.51800029e+00,  1.89954702e+00,  1.16494078e+00, -1.16441608e+00,\n",
       "       -2.29193975e-01, -5.45792512e-01,  6.34280687e-02, -6.00300470e-02,\n",
       "        1.63790255e+00,  1.09773205e-02,  1.14289258e-02, -3.59120631e-01,\n",
       "        1.64882523e+00,  1.83996634e+00, -2.22533703e+00, -2.63995934e-02,\n",
       "        7.39683479e-01, -9.47871073e-01, -6.96603201e-01,  6.73945014e-01,\n",
       "        6.83980493e-01,  1.85907055e+00, -1.02730453e-03, -7.11148876e-01,\n",
       "        1.34845376e+00,  1.68042049e+00, -6.32478631e-01, -6.88250308e-01,\n",
       "        3.21382321e+00,  2.48434832e+00, -2.23689454e-01, -2.01307663e+00,\n",
       "        5.56109620e-01, -1.14832534e+00, -3.31298067e+00, -1.06003216e+00,\n",
       "        1.27841393e+00,  3.57360732e+00, -3.94011376e-02, -9.28201266e-01,\n",
       "       -5.21547202e-01, -3.49631883e-01, -5.87507421e-01, -2.60607002e+00,\n",
       "       -1.61761569e+00,  1.37961496e+00, -1.56569403e+00,  1.73579327e+00,\n",
       "        1.01182898e+00,  1.05390539e+00, -2.45975311e-01, -5.30559841e-01,\n",
       "        9.33281042e-01, -7.85803206e-01, -4.09092484e-01,  1.27984307e+00,\n",
       "       -4.39620467e-02])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "binary_model = BinaryLogistic()\n",
    "\n",
    "binary_model.fit(X_train, y_train)\n",
    "\n",
    "binary_model.theta_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct num 100\n",
      "accuracy 0.9900990099009901\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.022058743662158153"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('correct num', np.sum(binary_model.prob(X_test) == y_test))\n",
    "\n",
    "print('accuracy', np.mean(binary_model.prob(X_test) == y_test))\n",
    "\n",
    "binary_model.crossEntropyLoss(X_test, y_test, binary_model.theta_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[47.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 54.]]),\n",
       " array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n",
       "        0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ]),\n",
       " <a list of 2 BarContainer objects>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "yhat = binary_model.prob(X_test)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "bins = np.linspace(0, 1, 10)\n",
    "ax.hist([binary_model.p[yhat == 0], binary_model.p[yhat == 1]], bins)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y_i = \\left[\\matrix{y_{i1}\\\\y_{i2}\\\\...\\\\y_{ik}}\\right] \\rightarrow Softmax \\rightarrow \\left[\\matrix{p_{i1}\\\\p_{i2}\\\\...\\\\p_{ik}}\\right]\n",
    "\\\\\n",
    "L(\\theta) = - \\left[ \\sum_{i=1}^{m} \\sum_{k=1}^{K}  1\\left\\{y_{i} = k\\right\\} \\log \\frac{\\exp(\\theta^{(k)\\top} \\mathbf{x}_i)}{\\sum_{j=1}^K \\exp(\\theta^{(j)\\top} \\mathbf{x}_i)}\\right]\n",
    "\\\\\n",
    "\\nabla_{\\theta^{(k)}} L(\\theta) = - \\sum_{i=1}^{m}{ \\left[ \\mathbf{x}_i \\left( 1\\{ y_i = k\\}  - P(y_i = k | \\mathbf{x}_i; \\theta) \\right) \\right]  }\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "X = N x D\n",
    "theta = D x K\n",
    "y, N x K, one hot encoding\n",
    "\n",
    "Z = X @ theta, N x K\n",
    "\"\"\"\n",
    "\n",
    "def get_one_hot(targets, nb_classes):\n",
    "    res = np.eye(nb_classes)[np.array(targets).reshape(-1)]\n",
    "    return res.reshape(list(targets.shape)+[nb_classes])\n",
    "\n",
    "def softmax(Z):\n",
    "    return np.exp(Z) / np.exp(Z).sum(1).reshape(-1, 1)\n",
    "\n",
    "\n",
    "def softmax_loss(X, y, theta):\n",
    "    p = softmax(X @ theta)\n",
    "    return -np.trace(np.log(p) @ y.T)\n",
    "\n",
    "\n",
    "def softmax_gradient(X, y, theta):\n",
    "    p = softmax(X @ theta)\n",
    "    return - X.T @(y - p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1293, 65), (1293, 10), (504, 65))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 1\n",
    "digits = datasets.load_digits(10) \n",
    "\n",
    "X, y = digits.data, digits.target\n",
    "X = np.c_[np.ones(X.shape[0]), X]\n",
    "X /= 255\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.28)\n",
    "y_train = get_one_hot(y_train, 10)\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss:  2856.129938971855\n",
      "Training Loss:  517.4721114519238\n",
      "Training Loss:  345.90147572277914\n",
      "Training Loss:  276.81652058584007\n",
      "Training Loss:  237.47493401527385\n",
      "Training Loss:  211.2931293144157\n",
      "Training Loss:  192.2220692608446\n",
      "Training Loss:  177.48559215078217\n",
      "Training Loss:  165.61643385877508\n",
      "Training Loss:  155.7601622008176\n",
      "Training Loss:  147.38252779452506\n",
      "Training Loss:  140.13073481586198\n",
      "Training Loss:  133.76149095870562\n",
      "Training Loss:  128.1009739368276\n",
      "Training Loss:  123.02125844299945\n",
      "Training Loss:  118.4257740795297\n",
      "Training Loss:  114.23998073175638\n",
      "Training Loss:  110.40519378008105\n",
      "Training Loss:  106.87438422783967\n",
      "Training Loss:  103.60925774844804\n",
      "\n",
      "accuracy: 0.9623015873015873\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.1\n",
    "theta_gd = np.random.randn(X_train.shape[1], 10)\n",
    "\n",
    "for e in range(0, 2000):\n",
    "    gr = softmax_gradient(X_train, y_train, theta_gd)\n",
    "    theta_gd -= alpha * gr\n",
    "\n",
    "    if e%100 == 0:\n",
    "        print(\"Training Loss: \", softmax_loss(X_train, y_train, theta_gd))\n",
    "\n",
    "# # Compute the accuracy of the test set\n",
    "proba = softmax(X_test @ theta_gd)\n",
    "\n",
    "print()\n",
    "print('accuracy:', float((proba.argmax(1) - y_test==0).sum()) / float(proba.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((178, 14), (178, 3))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 2: import data\n",
    "wine = datasets.load_wine()\n",
    "X = wine['data']\n",
    "y = wine['target']\n",
    "\n",
    "def standard_scaler(X):\n",
    "    mean = X.mean(0)\n",
    "    sd = X.std(0)\n",
    "    return (X - mean)/sd \n",
    "\n",
    "K = len(np.unique(y))\n",
    "X = standard_scaler(X) \n",
    "X_train = np.c_[np.ones(X.shape[0]), X]\n",
    "y_train = get_one_hot(y, K)\n",
    "\n",
    "X_train.shape, y_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss:  636.4510125318758\n",
      "Training Loss:  13.764993856288132\n",
      "Training Loss:  8.064571203366159\n",
      "Training Loss:  6.027095731402371\n",
      "Training Loss:  4.93139319652375\n",
      "Training Loss:  4.227586948348744\n",
      "Training Loss:  3.727757194719059\n",
      "Training Loss:  3.349416442277416\n",
      "Training Loss:  3.0503178038979253\n",
      "Training Loss:  2.8063675748083954\n",
      "accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "alpha = .0001\n",
    "theta_gd = np.random.randn(X_train.shape[1], K)\n",
    "\n",
    "for e in range(0, 10000):\n",
    "    gr = softmax_gradient(X_train, y_train, theta_gd)\n",
    "    theta_gd -= alpha * gr\n",
    "\n",
    "    if e%1000 == 0:\n",
    "        print(\"Training Loss: \", softmax_loss(X_train, y_train, theta_gd))\n",
    "\n",
    "# # Compute the accuracy of the test set\n",
    "proba = softmax(X_train @ theta_gd)\n",
    "\n",
    "print('accuracy:', float((proba.argmax(1) - y ==0).sum()) / float(proba.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
